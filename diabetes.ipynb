{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diabetes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDegjWWa3Vde1QHqnq4/Cy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadikinisaac/diabetes/blob/master/diabetes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esT_ON3bKUjV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "6499557e-830e-41e5-f191-3111bba18334"
      },
      "source": [
        "#Load libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOX_3VEgK_CP",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "ede8859e-e7db-4773-dc2d-ece3d73e47f1"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8b9201bc-9880-4545-a25b-aed0b777a239\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8b9201bc-9880-4545-a25b-aed0b777a239\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving diabetes.csv to diabetes.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWyqIPvZLCVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "61ef93d9-4569-41e8-8de0-3338c8b4494c"
      },
      "source": [
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['diabetes.csv']))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joZdz75ALLgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb5dbc87-ef4e-421a-9f1b-569f0cd11389"
      },
      "source": [
        "#Show the shape (number of rows & columns)\n",
        "df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9UYamEwLOxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checking for duplicates and removing them\n",
        "df.drop_duplicates(inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVIcvlJiLRI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b35f5e8d-175e-4a0c-cbed-163d524e5fe8"
      },
      "source": [
        "#Show the shape to see if any rows were dropped (number of rows & columns)\n",
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez2sfSCDLTY5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0128d80f-b4be-4688-9539-519182d0f71c"
      },
      "source": [
        "#Show the number of missing (NAN, NaN, na) data for each column\n",
        "df.isnull().sum()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pregnancies                 0\n",
              "Glucose                     0\n",
              "BloodPressure               0\n",
              "SkinThickness               0\n",
              "Insulin                     0\n",
              "BMI                         0\n",
              "DiabetesPedigreeFunction    0\n",
              "Age                         0\n",
              "Outcome                     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hTShN9DLcWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f278f452-18ca-4797-ceb5-49b117337d70"
      },
      "source": [
        "#Convert the data into an array\n",
        "dataset = df.values\n",
        "dataset"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
              "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
              "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
              "       ...,\n",
              "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
              "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
              "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFh9hRgiLe2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all of the rows from the first eight columns of the dataset\n",
        "X = dataset[:,0:8] #X = dataset[:,0:8]   #X = df.iloc[:, 0:8] \n",
        "# Get all of the rows from the last column\n",
        "y = dataset[:,8] #y = dataset[:,8]     #y = df.iloc[:, 8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThoiJ9_kLihL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c350770d-66f1-40be-9e21-9c17db460950"
      },
      "source": [
        "#Process the data\n",
        "#the min-max scaler method scales the dataset so that all the input features lie between 0 and 1 inclusive\n",
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = min_max_scaler.fit_transform(X)\n",
        "X_scale"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35294118, 0.74371859, 0.59016393, ..., 0.50074516, 0.23441503,\n",
              "        0.48333333],\n",
              "       [0.05882353, 0.42713568, 0.54098361, ..., 0.39642325, 0.11656704,\n",
              "        0.16666667],\n",
              "       [0.47058824, 0.91959799, 0.52459016, ..., 0.34724292, 0.25362938,\n",
              "        0.18333333],\n",
              "       ...,\n",
              "       [0.29411765, 0.6080402 , 0.59016393, ..., 0.390462  , 0.07130658,\n",
              "        0.15      ],\n",
              "       [0.05882353, 0.63316583, 0.49180328, ..., 0.4485842 , 0.11571307,\n",
              "        0.43333333],\n",
              "       [0.05882353, 0.46733668, 0.57377049, ..., 0.45305514, 0.10119556,\n",
              "        0.03333333]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpvdysfnLk7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data into 80% training and 20%\n",
        "\n",
        "#train_test_split splits arrays or matrices into random train and test subsets. \n",
        "#That means that everytime you run it without specifying random_state, you will get a different result, this is expected behavior.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.2, random_state = 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPIvxVipLoFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "7b349ccf-28fe-4c36-edfa-b88ff4556850"
      },
      "source": [
        "#Build the model and architecture of the neural network\n",
        "\n",
        "# The models architechture 3 layers,\n",
        "# 1st layer with 12 neurons and activation function 'relu'\n",
        "# 2nd layer with 15 neurons and activation function 'relu'\n",
        "# the last layer has 1 neuron with an activation function = sigmoid function which returns a value btwn 0 and 1\n",
        "# The input shape/ input_dim = 8 the number of features in the data set\n",
        "model = Sequential([\n",
        "    Dense(12, activation='relu', input_shape=( 8 ,)),\n",
        "    Dense(15, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_yj23iQLqaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "50c823a8-0197-41a1-e21e-c03c3327698f"
      },
      "source": [
        "# Loss measures how well the model did on training , and then tries to improve on it using the optimizer\n",
        "model.compile(optimizer='sgd', #Stochastic gradient descent optimizer.\n",
        "              loss='binary_crossentropy', #Used for binary classification\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98gklmeXL17U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e819016-9c46-4dc0-83f2-0bd68bd56838"
      },
      "source": [
        "#Train the model\n",
        "\n",
        "# Split the data into 20% validation data\n",
        "hist = model.fit(X_train, y_train,\n",
        "          batch_size=57, epochs=1000, validation_split=0.2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 491 samples, validate on 123 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "491/491 [==============================] - 1s 1ms/step - loss: 0.6882 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.6423\n",
            "Epoch 2/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6852 - acc: 0.6477 - val_loss: 0.6814 - val_acc: 0.6504\n",
            "Epoch 3/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6825 - acc: 0.6477 - val_loss: 0.6786 - val_acc: 0.6504\n",
            "Epoch 4/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6800 - acc: 0.6477 - val_loss: 0.6761 - val_acc: 0.6504\n",
            "Epoch 5/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6778 - acc: 0.6477 - val_loss: 0.6739 - val_acc: 0.6504\n",
            "Epoch 6/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6758 - acc: 0.6477 - val_loss: 0.6719 - val_acc: 0.6504\n",
            "Epoch 7/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6740 - acc: 0.6477 - val_loss: 0.6701 - val_acc: 0.6504\n",
            "Epoch 8/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6725 - acc: 0.6477 - val_loss: 0.6683 - val_acc: 0.6504\n",
            "Epoch 9/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6709 - acc: 0.6477 - val_loss: 0.6668 - val_acc: 0.6504\n",
            "Epoch 10/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6695 - acc: 0.6477 - val_loss: 0.6655 - val_acc: 0.6504\n",
            "Epoch 11/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6682 - acc: 0.6477 - val_loss: 0.6641 - val_acc: 0.6504\n",
            "Epoch 12/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6670 - acc: 0.6477 - val_loss: 0.6630 - val_acc: 0.6504\n",
            "Epoch 13/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6659 - acc: 0.6477 - val_loss: 0.6619 - val_acc: 0.6504\n",
            "Epoch 14/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6650 - acc: 0.6477 - val_loss: 0.6610 - val_acc: 0.6504\n",
            "Epoch 15/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6642 - acc: 0.6477 - val_loss: 0.6601 - val_acc: 0.6504\n",
            "Epoch 16/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6633 - acc: 0.6477 - val_loss: 0.6593 - val_acc: 0.6504\n",
            "Epoch 17/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6625 - acc: 0.6477 - val_loss: 0.6586 - val_acc: 0.6504\n",
            "Epoch 18/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6619 - acc: 0.6477 - val_loss: 0.6579 - val_acc: 0.6504\n",
            "Epoch 19/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6612 - acc: 0.6477 - val_loss: 0.6572 - val_acc: 0.6504\n",
            "Epoch 20/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6605 - acc: 0.6477 - val_loss: 0.6565 - val_acc: 0.6504\n",
            "Epoch 21/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6599 - acc: 0.6477 - val_loss: 0.6560 - val_acc: 0.6504\n",
            "Epoch 22/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6594 - acc: 0.6477 - val_loss: 0.6554 - val_acc: 0.6504\n",
            "Epoch 23/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6588 - acc: 0.6477 - val_loss: 0.6549 - val_acc: 0.6504\n",
            "Epoch 24/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.6584 - acc: 0.6477 - val_loss: 0.6544 - val_acc: 0.6504\n",
            "Epoch 25/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.6579 - acc: 0.6477 - val_loss: 0.6540 - val_acc: 0.6504\n",
            "Epoch 26/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6574 - acc: 0.6477 - val_loss: 0.6535 - val_acc: 0.6504\n",
            "Epoch 27/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6570 - acc: 0.6477 - val_loss: 0.6532 - val_acc: 0.6504\n",
            "Epoch 28/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6566 - acc: 0.6477 - val_loss: 0.6528 - val_acc: 0.6504\n",
            "Epoch 29/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6563 - acc: 0.6477 - val_loss: 0.6525 - val_acc: 0.6504\n",
            "Epoch 30/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6559 - acc: 0.6477 - val_loss: 0.6522 - val_acc: 0.6504\n",
            "Epoch 31/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6555 - acc: 0.6477 - val_loss: 0.6518 - val_acc: 0.6504\n",
            "Epoch 32/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6553 - acc: 0.6477 - val_loss: 0.6515 - val_acc: 0.6504\n",
            "Epoch 33/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6549 - acc: 0.6477 - val_loss: 0.6512 - val_acc: 0.6504\n",
            "Epoch 34/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.6546 - acc: 0.6477 - val_loss: 0.6509 - val_acc: 0.6504\n",
            "Epoch 35/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6543 - acc: 0.6477 - val_loss: 0.6506 - val_acc: 0.6504\n",
            "Epoch 36/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6540 - acc: 0.6477 - val_loss: 0.6504 - val_acc: 0.6504\n",
            "Epoch 37/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.6537 - acc: 0.6477 - val_loss: 0.6501 - val_acc: 0.6504\n",
            "Epoch 38/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.6534 - acc: 0.6477 - val_loss: 0.6498 - val_acc: 0.6504\n",
            "Epoch 39/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6532 - acc: 0.6477 - val_loss: 0.6496 - val_acc: 0.6504\n",
            "Epoch 40/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6529 - acc: 0.6477 - val_loss: 0.6493 - val_acc: 0.6504\n",
            "Epoch 41/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6526 - acc: 0.6477 - val_loss: 0.6490 - val_acc: 0.6504\n",
            "Epoch 42/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6523 - acc: 0.6477 - val_loss: 0.6488 - val_acc: 0.6504\n",
            "Epoch 43/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.6520 - acc: 0.6477 - val_loss: 0.6485 - val_acc: 0.6504\n",
            "Epoch 44/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6518 - acc: 0.6477 - val_loss: 0.6483 - val_acc: 0.6504\n",
            "Epoch 45/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6516 - acc: 0.6477 - val_loss: 0.6481 - val_acc: 0.6504\n",
            "Epoch 46/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6514 - acc: 0.6477 - val_loss: 0.6479 - val_acc: 0.6504\n",
            "Epoch 47/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6511 - acc: 0.6477 - val_loss: 0.6476 - val_acc: 0.6504\n",
            "Epoch 48/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6509 - acc: 0.6477 - val_loss: 0.6475 - val_acc: 0.6504\n",
            "Epoch 49/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6507 - acc: 0.6477 - val_loss: 0.6472 - val_acc: 0.6504\n",
            "Epoch 50/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6504 - acc: 0.6477 - val_loss: 0.6470 - val_acc: 0.6504\n",
            "Epoch 51/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6502 - acc: 0.6477 - val_loss: 0.6468 - val_acc: 0.6504\n",
            "Epoch 52/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6500 - acc: 0.6477 - val_loss: 0.6466 - val_acc: 0.6504\n",
            "Epoch 53/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6498 - acc: 0.6477 - val_loss: 0.6464 - val_acc: 0.6504\n",
            "Epoch 54/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6496 - acc: 0.6477 - val_loss: 0.6463 - val_acc: 0.6504\n",
            "Epoch 55/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.6494 - acc: 0.6477 - val_loss: 0.6461 - val_acc: 0.6504\n",
            "Epoch 56/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6492 - acc: 0.6477 - val_loss: 0.6459 - val_acc: 0.6504\n",
            "Epoch 57/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6490 - acc: 0.6477 - val_loss: 0.6457 - val_acc: 0.6504\n",
            "Epoch 58/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6487 - acc: 0.6477 - val_loss: 0.6455 - val_acc: 0.6504\n",
            "Epoch 59/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6486 - acc: 0.6477 - val_loss: 0.6454 - val_acc: 0.6504\n",
            "Epoch 60/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6484 - acc: 0.6477 - val_loss: 0.6452 - val_acc: 0.6504\n",
            "Epoch 61/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6482 - acc: 0.6477 - val_loss: 0.6451 - val_acc: 0.6504\n",
            "Epoch 62/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6479 - acc: 0.6477 - val_loss: 0.6449 - val_acc: 0.6504\n",
            "Epoch 63/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6478 - acc: 0.6477 - val_loss: 0.6447 - val_acc: 0.6504\n",
            "Epoch 64/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.6476 - acc: 0.6477 - val_loss: 0.6446 - val_acc: 0.6504\n",
            "Epoch 65/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6474 - acc: 0.6477 - val_loss: 0.6444 - val_acc: 0.6504\n",
            "Epoch 66/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6472 - acc: 0.6477 - val_loss: 0.6442 - val_acc: 0.6504\n",
            "Epoch 67/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6470 - acc: 0.6477 - val_loss: 0.6441 - val_acc: 0.6504\n",
            "Epoch 68/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6468 - acc: 0.6477 - val_loss: 0.6439 - val_acc: 0.6504\n",
            "Epoch 69/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6467 - acc: 0.6477 - val_loss: 0.6437 - val_acc: 0.6504\n",
            "Epoch 70/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6465 - acc: 0.6477 - val_loss: 0.6436 - val_acc: 0.6504\n",
            "Epoch 71/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6463 - acc: 0.6477 - val_loss: 0.6434 - val_acc: 0.6504\n",
            "Epoch 72/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6461 - acc: 0.6477 - val_loss: 0.6433 - val_acc: 0.6504\n",
            "Epoch 73/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6459 - acc: 0.6477 - val_loss: 0.6431 - val_acc: 0.6504\n",
            "Epoch 74/1000\n",
            "491/491 [==============================] - 0s 52us/step - loss: 0.6457 - acc: 0.6477 - val_loss: 0.6430 - val_acc: 0.6504\n",
            "Epoch 75/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6455 - acc: 0.6477 - val_loss: 0.6428 - val_acc: 0.6504\n",
            "Epoch 76/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6454 - acc: 0.6477 - val_loss: 0.6426 - val_acc: 0.6504\n",
            "Epoch 77/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6451 - acc: 0.6477 - val_loss: 0.6425 - val_acc: 0.6504\n",
            "Epoch 78/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6449 - acc: 0.6477 - val_loss: 0.6423 - val_acc: 0.6504\n",
            "Epoch 79/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6448 - acc: 0.6477 - val_loss: 0.6421 - val_acc: 0.6504\n",
            "Epoch 80/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6445 - acc: 0.6477 - val_loss: 0.6420 - val_acc: 0.6504\n",
            "Epoch 81/1000\n",
            "491/491 [==============================] - 0s 51us/step - loss: 0.6443 - acc: 0.6477 - val_loss: 0.6418 - val_acc: 0.6504\n",
            "Epoch 82/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.6441 - acc: 0.6477 - val_loss: 0.6416 - val_acc: 0.6504\n",
            "Epoch 83/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6439 - acc: 0.6477 - val_loss: 0.6415 - val_acc: 0.6504\n",
            "Epoch 84/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.6437 - acc: 0.6477 - val_loss: 0.6413 - val_acc: 0.6504\n",
            "Epoch 85/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6436 - acc: 0.6477 - val_loss: 0.6411 - val_acc: 0.6504\n",
            "Epoch 86/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6434 - acc: 0.6477 - val_loss: 0.6410 - val_acc: 0.6504\n",
            "Epoch 87/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6431 - acc: 0.6477 - val_loss: 0.6408 - val_acc: 0.6504\n",
            "Epoch 88/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6429 - acc: 0.6477 - val_loss: 0.6406 - val_acc: 0.6504\n",
            "Epoch 89/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6427 - acc: 0.6477 - val_loss: 0.6405 - val_acc: 0.6504\n",
            "Epoch 90/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6425 - acc: 0.6477 - val_loss: 0.6403 - val_acc: 0.6504\n",
            "Epoch 91/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6423 - acc: 0.6477 - val_loss: 0.6401 - val_acc: 0.6504\n",
            "Epoch 92/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.6421 - acc: 0.6477 - val_loss: 0.6400 - val_acc: 0.6504\n",
            "Epoch 93/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.6420 - acc: 0.6477 - val_loss: 0.6398 - val_acc: 0.6504\n",
            "Epoch 94/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6418 - acc: 0.6477 - val_loss: 0.6397 - val_acc: 0.6504\n",
            "Epoch 95/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6416 - acc: 0.6477 - val_loss: 0.6395 - val_acc: 0.6504\n",
            "Epoch 96/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6413 - acc: 0.6477 - val_loss: 0.6394 - val_acc: 0.6504\n",
            "Epoch 97/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6411 - acc: 0.6477 - val_loss: 0.6392 - val_acc: 0.6504\n",
            "Epoch 98/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6409 - acc: 0.6477 - val_loss: 0.6390 - val_acc: 0.6504\n",
            "Epoch 99/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6408 - acc: 0.6477 - val_loss: 0.6389 - val_acc: 0.6504\n",
            "Epoch 100/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6405 - acc: 0.6477 - val_loss: 0.6387 - val_acc: 0.6504\n",
            "Epoch 101/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6404 - acc: 0.6477 - val_loss: 0.6385 - val_acc: 0.6504\n",
            "Epoch 102/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6402 - acc: 0.6477 - val_loss: 0.6384 - val_acc: 0.6504\n",
            "Epoch 103/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6400 - acc: 0.6477 - val_loss: 0.6382 - val_acc: 0.6504\n",
            "Epoch 104/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6397 - acc: 0.6477 - val_loss: 0.6380 - val_acc: 0.6504\n",
            "Epoch 105/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6396 - acc: 0.6477 - val_loss: 0.6379 - val_acc: 0.6504\n",
            "Epoch 106/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6394 - acc: 0.6477 - val_loss: 0.6377 - val_acc: 0.6504\n",
            "Epoch 107/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6392 - acc: 0.6477 - val_loss: 0.6375 - val_acc: 0.6504\n",
            "Epoch 108/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6390 - acc: 0.6477 - val_loss: 0.6374 - val_acc: 0.6504\n",
            "Epoch 109/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6388 - acc: 0.6477 - val_loss: 0.6372 - val_acc: 0.6504\n",
            "Epoch 110/1000\n",
            "491/491 [==============================] - 0s 53us/step - loss: 0.6385 - acc: 0.6477 - val_loss: 0.6371 - val_acc: 0.6504\n",
            "Epoch 111/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.6384 - acc: 0.6477 - val_loss: 0.6369 - val_acc: 0.6504\n",
            "Epoch 112/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6382 - acc: 0.6477 - val_loss: 0.6367 - val_acc: 0.6504\n",
            "Epoch 113/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6381 - acc: 0.6477 - val_loss: 0.6366 - val_acc: 0.6504\n",
            "Epoch 114/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.6377 - acc: 0.6477 - val_loss: 0.6364 - val_acc: 0.6504\n",
            "Epoch 115/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6376 - acc: 0.6477 - val_loss: 0.6363 - val_acc: 0.6504\n",
            "Epoch 116/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6374 - acc: 0.6477 - val_loss: 0.6361 - val_acc: 0.6504\n",
            "Epoch 117/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6372 - acc: 0.6477 - val_loss: 0.6359 - val_acc: 0.6504\n",
            "Epoch 118/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6370 - acc: 0.6477 - val_loss: 0.6357 - val_acc: 0.6504\n",
            "Epoch 119/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6367 - acc: 0.6477 - val_loss: 0.6356 - val_acc: 0.6504\n",
            "Epoch 120/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.6365 - acc: 0.6477 - val_loss: 0.6354 - val_acc: 0.6504\n",
            "Epoch 121/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.6363 - acc: 0.6477 - val_loss: 0.6352 - val_acc: 0.6504\n",
            "Epoch 122/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6361 - acc: 0.6477 - val_loss: 0.6351 - val_acc: 0.6504\n",
            "Epoch 123/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6360 - acc: 0.6477 - val_loss: 0.6349 - val_acc: 0.6504\n",
            "Epoch 124/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6357 - acc: 0.6477 - val_loss: 0.6347 - val_acc: 0.6504\n",
            "Epoch 125/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6355 - acc: 0.6477 - val_loss: 0.6345 - val_acc: 0.6504\n",
            "Epoch 126/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6353 - acc: 0.6477 - val_loss: 0.6343 - val_acc: 0.6504\n",
            "Epoch 127/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6351 - acc: 0.6477 - val_loss: 0.6342 - val_acc: 0.6504\n",
            "Epoch 128/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6349 - acc: 0.6477 - val_loss: 0.6340 - val_acc: 0.6504\n",
            "Epoch 129/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.6347 - acc: 0.6477 - val_loss: 0.6338 - val_acc: 0.6504\n",
            "Epoch 130/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.6344 - acc: 0.6477 - val_loss: 0.6337 - val_acc: 0.6504\n",
            "Epoch 131/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6342 - acc: 0.6477 - val_loss: 0.6335 - val_acc: 0.6504\n",
            "Epoch 132/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6340 - acc: 0.6477 - val_loss: 0.6333 - val_acc: 0.6504\n",
            "Epoch 133/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6338 - acc: 0.6477 - val_loss: 0.6331 - val_acc: 0.6504\n",
            "Epoch 134/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6335 - acc: 0.6477 - val_loss: 0.6329 - val_acc: 0.6504\n",
            "Epoch 135/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6333 - acc: 0.6477 - val_loss: 0.6328 - val_acc: 0.6504\n",
            "Epoch 136/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6331 - acc: 0.6477 - val_loss: 0.6326 - val_acc: 0.6504\n",
            "Epoch 137/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6329 - acc: 0.6477 - val_loss: 0.6324 - val_acc: 0.6504\n",
            "Epoch 138/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6327 - acc: 0.6477 - val_loss: 0.6322 - val_acc: 0.6504\n",
            "Epoch 139/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6326 - acc: 0.6477 - val_loss: 0.6320 - val_acc: 0.6504\n",
            "Epoch 140/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6322 - acc: 0.6477 - val_loss: 0.6318 - val_acc: 0.6504\n",
            "Epoch 141/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6320 - acc: 0.6477 - val_loss: 0.6316 - val_acc: 0.6504\n",
            "Epoch 142/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6317 - acc: 0.6477 - val_loss: 0.6314 - val_acc: 0.6504\n",
            "Epoch 143/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6315 - acc: 0.6477 - val_loss: 0.6312 - val_acc: 0.6504\n",
            "Epoch 144/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6312 - acc: 0.6477 - val_loss: 0.6310 - val_acc: 0.6504\n",
            "Epoch 145/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6310 - acc: 0.6477 - val_loss: 0.6308 - val_acc: 0.6504\n",
            "Epoch 146/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6308 - acc: 0.6477 - val_loss: 0.6306 - val_acc: 0.6504\n",
            "Epoch 147/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.6305 - acc: 0.6477 - val_loss: 0.6304 - val_acc: 0.6504\n",
            "Epoch 148/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6304 - acc: 0.6477 - val_loss: 0.6301 - val_acc: 0.6504\n",
            "Epoch 149/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.6301 - acc: 0.6477 - val_loss: 0.6299 - val_acc: 0.6504\n",
            "Epoch 150/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6298 - acc: 0.6477 - val_loss: 0.6297 - val_acc: 0.6504\n",
            "Epoch 151/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6295 - acc: 0.6477 - val_loss: 0.6295 - val_acc: 0.6504\n",
            "Epoch 152/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6293 - acc: 0.6477 - val_loss: 0.6293 - val_acc: 0.6504\n",
            "Epoch 153/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6291 - acc: 0.6477 - val_loss: 0.6291 - val_acc: 0.6504\n",
            "Epoch 154/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6288 - acc: 0.6477 - val_loss: 0.6289 - val_acc: 0.6504\n",
            "Epoch 155/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6285 - acc: 0.6477 - val_loss: 0.6286 - val_acc: 0.6504\n",
            "Epoch 156/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6282 - acc: 0.6477 - val_loss: 0.6284 - val_acc: 0.6504\n",
            "Epoch 157/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6279 - acc: 0.6477 - val_loss: 0.6282 - val_acc: 0.6504\n",
            "Epoch 158/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6277 - acc: 0.6477 - val_loss: 0.6279 - val_acc: 0.6504\n",
            "Epoch 159/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6274 - acc: 0.6477 - val_loss: 0.6276 - val_acc: 0.6504\n",
            "Epoch 160/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6270 - acc: 0.6477 - val_loss: 0.6274 - val_acc: 0.6504\n",
            "Epoch 161/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6268 - acc: 0.6477 - val_loss: 0.6272 - val_acc: 0.6504\n",
            "Epoch 162/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6266 - acc: 0.6477 - val_loss: 0.6269 - val_acc: 0.6504\n",
            "Epoch 163/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6262 - acc: 0.6477 - val_loss: 0.6266 - val_acc: 0.6504\n",
            "Epoch 164/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6260 - acc: 0.6477 - val_loss: 0.6264 - val_acc: 0.6504\n",
            "Epoch 165/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6257 - acc: 0.6477 - val_loss: 0.6262 - val_acc: 0.6504\n",
            "Epoch 166/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6254 - acc: 0.6477 - val_loss: 0.6259 - val_acc: 0.6504\n",
            "Epoch 167/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.6251 - acc: 0.6477 - val_loss: 0.6257 - val_acc: 0.6504\n",
            "Epoch 168/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6248 - acc: 0.6477 - val_loss: 0.6254 - val_acc: 0.6504\n",
            "Epoch 169/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6245 - acc: 0.6477 - val_loss: 0.6252 - val_acc: 0.6504\n",
            "Epoch 170/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6243 - acc: 0.6477 - val_loss: 0.6250 - val_acc: 0.6504\n",
            "Epoch 171/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6240 - acc: 0.6477 - val_loss: 0.6247 - val_acc: 0.6504\n",
            "Epoch 172/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6237 - acc: 0.6477 - val_loss: 0.6245 - val_acc: 0.6504\n",
            "Epoch 173/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6234 - acc: 0.6477 - val_loss: 0.6242 - val_acc: 0.6504\n",
            "Epoch 174/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6232 - acc: 0.6477 - val_loss: 0.6240 - val_acc: 0.6504\n",
            "Epoch 175/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6229 - acc: 0.6477 - val_loss: 0.6237 - val_acc: 0.6504\n",
            "Epoch 176/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6225 - acc: 0.6477 - val_loss: 0.6235 - val_acc: 0.6504\n",
            "Epoch 177/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6223 - acc: 0.6477 - val_loss: 0.6232 - val_acc: 0.6504\n",
            "Epoch 178/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6220 - acc: 0.6477 - val_loss: 0.6230 - val_acc: 0.6504\n",
            "Epoch 179/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.6217 - acc: 0.6477 - val_loss: 0.6227 - val_acc: 0.6504\n",
            "Epoch 180/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6213 - acc: 0.6477 - val_loss: 0.6225 - val_acc: 0.6504\n",
            "Epoch 181/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6211 - acc: 0.6497 - val_loss: 0.6223 - val_acc: 0.6504\n",
            "Epoch 182/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6208 - acc: 0.6497 - val_loss: 0.6220 - val_acc: 0.6504\n",
            "Epoch 183/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6204 - acc: 0.6497 - val_loss: 0.6218 - val_acc: 0.6504\n",
            "Epoch 184/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6203 - acc: 0.6497 - val_loss: 0.6215 - val_acc: 0.6504\n",
            "Epoch 185/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6199 - acc: 0.6497 - val_loss: 0.6213 - val_acc: 0.6504\n",
            "Epoch 186/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6195 - acc: 0.6497 - val_loss: 0.6210 - val_acc: 0.6504\n",
            "Epoch 187/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6192 - acc: 0.6497 - val_loss: 0.6208 - val_acc: 0.6504\n",
            "Epoch 188/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6189 - acc: 0.6497 - val_loss: 0.6205 - val_acc: 0.6504\n",
            "Epoch 189/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6185 - acc: 0.6497 - val_loss: 0.6202 - val_acc: 0.6504\n",
            "Epoch 190/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6184 - acc: 0.6497 - val_loss: 0.6199 - val_acc: 0.6504\n",
            "Epoch 191/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6180 - acc: 0.6497 - val_loss: 0.6196 - val_acc: 0.6504\n",
            "Epoch 192/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6176 - acc: 0.6497 - val_loss: 0.6194 - val_acc: 0.6504\n",
            "Epoch 193/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6174 - acc: 0.6497 - val_loss: 0.6191 - val_acc: 0.6504\n",
            "Epoch 194/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6170 - acc: 0.6497 - val_loss: 0.6188 - val_acc: 0.6504\n",
            "Epoch 195/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6167 - acc: 0.6497 - val_loss: 0.6186 - val_acc: 0.6504\n",
            "Epoch 196/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6164 - acc: 0.6497 - val_loss: 0.6183 - val_acc: 0.6504\n",
            "Epoch 197/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6160 - acc: 0.6497 - val_loss: 0.6180 - val_acc: 0.6504\n",
            "Epoch 198/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6157 - acc: 0.6497 - val_loss: 0.6177 - val_acc: 0.6504\n",
            "Epoch 199/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.6153 - acc: 0.6497 - val_loss: 0.6175 - val_acc: 0.6504\n",
            "Epoch 200/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6151 - acc: 0.6477 - val_loss: 0.6172 - val_acc: 0.6504\n",
            "Epoch 201/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6147 - acc: 0.6477 - val_loss: 0.6169 - val_acc: 0.6504\n",
            "Epoch 202/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6144 - acc: 0.6517 - val_loss: 0.6166 - val_acc: 0.6504\n",
            "Epoch 203/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6140 - acc: 0.6517 - val_loss: 0.6164 - val_acc: 0.6504\n",
            "Epoch 204/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6137 - acc: 0.6517 - val_loss: 0.6161 - val_acc: 0.6504\n",
            "Epoch 205/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6133 - acc: 0.6517 - val_loss: 0.6158 - val_acc: 0.6504\n",
            "Epoch 206/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6130 - acc: 0.6517 - val_loss: 0.6155 - val_acc: 0.6504\n",
            "Epoch 207/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6126 - acc: 0.6517 - val_loss: 0.6152 - val_acc: 0.6504\n",
            "Epoch 208/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6123 - acc: 0.6517 - val_loss: 0.6149 - val_acc: 0.6504\n",
            "Epoch 209/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6119 - acc: 0.6538 - val_loss: 0.6146 - val_acc: 0.6504\n",
            "Epoch 210/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6116 - acc: 0.6538 - val_loss: 0.6142 - val_acc: 0.6504\n",
            "Epoch 211/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6113 - acc: 0.6538 - val_loss: 0.6139 - val_acc: 0.6504\n",
            "Epoch 212/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6108 - acc: 0.6538 - val_loss: 0.6136 - val_acc: 0.6504\n",
            "Epoch 213/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6105 - acc: 0.6538 - val_loss: 0.6133 - val_acc: 0.6504\n",
            "Epoch 214/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6102 - acc: 0.6578 - val_loss: 0.6130 - val_acc: 0.6504\n",
            "Epoch 215/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6098 - acc: 0.6538 - val_loss: 0.6127 - val_acc: 0.6504\n",
            "Epoch 216/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6095 - acc: 0.6578 - val_loss: 0.6124 - val_acc: 0.6504\n",
            "Epoch 217/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.6091 - acc: 0.6599 - val_loss: 0.6121 - val_acc: 0.6504\n",
            "Epoch 218/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6087 - acc: 0.6578 - val_loss: 0.6118 - val_acc: 0.6504\n",
            "Epoch 219/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6083 - acc: 0.6599 - val_loss: 0.6114 - val_acc: 0.6504\n",
            "Epoch 220/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6079 - acc: 0.6619 - val_loss: 0.6111 - val_acc: 0.6504\n",
            "Epoch 221/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6075 - acc: 0.6599 - val_loss: 0.6108 - val_acc: 0.6585\n",
            "Epoch 222/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6072 - acc: 0.6619 - val_loss: 0.6104 - val_acc: 0.6585\n",
            "Epoch 223/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6067 - acc: 0.6619 - val_loss: 0.6101 - val_acc: 0.6585\n",
            "Epoch 224/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6064 - acc: 0.6619 - val_loss: 0.6098 - val_acc: 0.6585\n",
            "Epoch 225/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6061 - acc: 0.6640 - val_loss: 0.6095 - val_acc: 0.6585\n",
            "Epoch 226/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6056 - acc: 0.6619 - val_loss: 0.6092 - val_acc: 0.6585\n",
            "Epoch 227/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6052 - acc: 0.6640 - val_loss: 0.6088 - val_acc: 0.6585\n",
            "Epoch 228/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6048 - acc: 0.6619 - val_loss: 0.6085 - val_acc: 0.6585\n",
            "Epoch 229/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6044 - acc: 0.6640 - val_loss: 0.6081 - val_acc: 0.6585\n",
            "Epoch 230/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6041 - acc: 0.6721 - val_loss: 0.6078 - val_acc: 0.6585\n",
            "Epoch 231/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6036 - acc: 0.6721 - val_loss: 0.6075 - val_acc: 0.6585\n",
            "Epoch 232/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6034 - acc: 0.6741 - val_loss: 0.6071 - val_acc: 0.6585\n",
            "Epoch 233/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6028 - acc: 0.6782 - val_loss: 0.6067 - val_acc: 0.6585\n",
            "Epoch 234/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6026 - acc: 0.6782 - val_loss: 0.6064 - val_acc: 0.6585\n",
            "Epoch 235/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6020 - acc: 0.6782 - val_loss: 0.6060 - val_acc: 0.6585\n",
            "Epoch 236/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6019 - acc: 0.6741 - val_loss: 0.6057 - val_acc: 0.6585\n",
            "Epoch 237/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6012 - acc: 0.6782 - val_loss: 0.6053 - val_acc: 0.6585\n",
            "Epoch 238/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6008 - acc: 0.6782 - val_loss: 0.6049 - val_acc: 0.6585\n",
            "Epoch 239/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6004 - acc: 0.6802 - val_loss: 0.6046 - val_acc: 0.6585\n",
            "Epoch 240/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6000 - acc: 0.6782 - val_loss: 0.6042 - val_acc: 0.6585\n",
            "Epoch 241/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5996 - acc: 0.6802 - val_loss: 0.6038 - val_acc: 0.6585\n",
            "Epoch 242/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5992 - acc: 0.6802 - val_loss: 0.6034 - val_acc: 0.6585\n",
            "Epoch 243/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5987 - acc: 0.6802 - val_loss: 0.6031 - val_acc: 0.6585\n",
            "Epoch 244/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5984 - acc: 0.6782 - val_loss: 0.6027 - val_acc: 0.6585\n",
            "Epoch 245/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5980 - acc: 0.6823 - val_loss: 0.6024 - val_acc: 0.6585\n",
            "Epoch 246/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5977 - acc: 0.6823 - val_loss: 0.6020 - val_acc: 0.6667\n",
            "Epoch 247/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5971 - acc: 0.6823 - val_loss: 0.6017 - val_acc: 0.6748\n",
            "Epoch 248/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5967 - acc: 0.6823 - val_loss: 0.6013 - val_acc: 0.6748\n",
            "Epoch 249/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5962 - acc: 0.6823 - val_loss: 0.6010 - val_acc: 0.6748\n",
            "Epoch 250/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5958 - acc: 0.6843 - val_loss: 0.6006 - val_acc: 0.6748\n",
            "Epoch 251/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5954 - acc: 0.6843 - val_loss: 0.6003 - val_acc: 0.6748\n",
            "Epoch 252/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5949 - acc: 0.6864 - val_loss: 0.5999 - val_acc: 0.6748\n",
            "Epoch 253/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5944 - acc: 0.6823 - val_loss: 0.5995 - val_acc: 0.6667\n",
            "Epoch 254/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5940 - acc: 0.6884 - val_loss: 0.5991 - val_acc: 0.6667\n",
            "Epoch 255/1000\n",
            "491/491 [==============================] - 0s 55us/step - loss: 0.5936 - acc: 0.6843 - val_loss: 0.5987 - val_acc: 0.6585\n",
            "Epoch 256/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5931 - acc: 0.6843 - val_loss: 0.5984 - val_acc: 0.6504\n",
            "Epoch 257/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5927 - acc: 0.6864 - val_loss: 0.5979 - val_acc: 0.6504\n",
            "Epoch 258/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5923 - acc: 0.6843 - val_loss: 0.5976 - val_acc: 0.6504\n",
            "Epoch 259/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5918 - acc: 0.6843 - val_loss: 0.5972 - val_acc: 0.6504\n",
            "Epoch 260/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.5916 - acc: 0.6823 - val_loss: 0.5967 - val_acc: 0.6504\n",
            "Epoch 261/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5908 - acc: 0.6843 - val_loss: 0.5964 - val_acc: 0.6504\n",
            "Epoch 262/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5903 - acc: 0.6864 - val_loss: 0.5960 - val_acc: 0.6504\n",
            "Epoch 263/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5899 - acc: 0.6864 - val_loss: 0.5956 - val_acc: 0.6504\n",
            "Epoch 264/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5896 - acc: 0.6925 - val_loss: 0.5952 - val_acc: 0.6504\n",
            "Epoch 265/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5891 - acc: 0.6904 - val_loss: 0.5947 - val_acc: 0.6504\n",
            "Epoch 266/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5885 - acc: 0.6884 - val_loss: 0.5944 - val_acc: 0.6504\n",
            "Epoch 267/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5881 - acc: 0.6884 - val_loss: 0.5940 - val_acc: 0.6504\n",
            "Epoch 268/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5876 - acc: 0.6904 - val_loss: 0.5937 - val_acc: 0.6504\n",
            "Epoch 269/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5871 - acc: 0.6925 - val_loss: 0.5933 - val_acc: 0.6504\n",
            "Epoch 270/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5867 - acc: 0.6904 - val_loss: 0.5929 - val_acc: 0.6423\n",
            "Epoch 271/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5862 - acc: 0.6925 - val_loss: 0.5925 - val_acc: 0.6423\n",
            "Epoch 272/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5858 - acc: 0.6965 - val_loss: 0.5921 - val_acc: 0.6423\n",
            "Epoch 273/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5853 - acc: 0.6965 - val_loss: 0.5917 - val_acc: 0.6423\n",
            "Epoch 274/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.5848 - acc: 0.6945 - val_loss: 0.5913 - val_acc: 0.6504\n",
            "Epoch 275/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5844 - acc: 0.7026 - val_loss: 0.5909 - val_acc: 0.6504\n",
            "Epoch 276/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5840 - acc: 0.6986 - val_loss: 0.5905 - val_acc: 0.6504\n",
            "Epoch 277/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5835 - acc: 0.7026 - val_loss: 0.5901 - val_acc: 0.6504\n",
            "Epoch 278/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5830 - acc: 0.7006 - val_loss: 0.5897 - val_acc: 0.6504\n",
            "Epoch 279/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5826 - acc: 0.6986 - val_loss: 0.5893 - val_acc: 0.6585\n",
            "Epoch 280/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.5820 - acc: 0.7026 - val_loss: 0.5888 - val_acc: 0.6585\n",
            "Epoch 281/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5815 - acc: 0.7047 - val_loss: 0.5885 - val_acc: 0.6585\n",
            "Epoch 282/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5811 - acc: 0.7006 - val_loss: 0.5881 - val_acc: 0.6585\n",
            "Epoch 283/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5806 - acc: 0.7026 - val_loss: 0.5876 - val_acc: 0.6585\n",
            "Epoch 284/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5802 - acc: 0.7006 - val_loss: 0.5872 - val_acc: 0.6585\n",
            "Epoch 285/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5798 - acc: 0.7067 - val_loss: 0.5867 - val_acc: 0.6585\n",
            "Epoch 286/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5795 - acc: 0.7026 - val_loss: 0.5863 - val_acc: 0.6585\n",
            "Epoch 287/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.5786 - acc: 0.6986 - val_loss: 0.5859 - val_acc: 0.6667\n",
            "Epoch 288/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5782 - acc: 0.7047 - val_loss: 0.5855 - val_acc: 0.6585\n",
            "Epoch 289/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5777 - acc: 0.7006 - val_loss: 0.5850 - val_acc: 0.6585\n",
            "Epoch 290/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5772 - acc: 0.7026 - val_loss: 0.5847 - val_acc: 0.6585\n",
            "Epoch 291/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5769 - acc: 0.7026 - val_loss: 0.5843 - val_acc: 0.6585\n",
            "Epoch 292/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5764 - acc: 0.7047 - val_loss: 0.5839 - val_acc: 0.6504\n",
            "Epoch 293/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.5758 - acc: 0.7047 - val_loss: 0.5835 - val_acc: 0.6585\n",
            "Epoch 294/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5756 - acc: 0.7108 - val_loss: 0.5831 - val_acc: 0.6667\n",
            "Epoch 295/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5749 - acc: 0.7169 - val_loss: 0.5826 - val_acc: 0.6585\n",
            "Epoch 296/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5743 - acc: 0.7088 - val_loss: 0.5823 - val_acc: 0.6667\n",
            "Epoch 297/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5741 - acc: 0.7047 - val_loss: 0.5819 - val_acc: 0.6667\n",
            "Epoch 298/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5735 - acc: 0.7189 - val_loss: 0.5815 - val_acc: 0.6667\n",
            "Epoch 299/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5730 - acc: 0.7149 - val_loss: 0.5810 - val_acc: 0.6667\n",
            "Epoch 300/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5724 - acc: 0.7128 - val_loss: 0.5806 - val_acc: 0.6667\n",
            "Epoch 301/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5720 - acc: 0.7169 - val_loss: 0.5803 - val_acc: 0.6667\n",
            "Epoch 302/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5714 - acc: 0.7210 - val_loss: 0.5798 - val_acc: 0.6667\n",
            "Epoch 303/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5709 - acc: 0.7210 - val_loss: 0.5794 - val_acc: 0.6667\n",
            "Epoch 304/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5706 - acc: 0.7189 - val_loss: 0.5790 - val_acc: 0.6667\n",
            "Epoch 305/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5699 - acc: 0.7210 - val_loss: 0.5786 - val_acc: 0.6667\n",
            "Epoch 306/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5694 - acc: 0.7189 - val_loss: 0.5782 - val_acc: 0.6667\n",
            "Epoch 307/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5692 - acc: 0.7189 - val_loss: 0.5778 - val_acc: 0.6748\n",
            "Epoch 308/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5684 - acc: 0.7189 - val_loss: 0.5774 - val_acc: 0.6748\n",
            "Epoch 309/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5682 - acc: 0.7169 - val_loss: 0.5770 - val_acc: 0.6748\n",
            "Epoch 310/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5675 - acc: 0.7210 - val_loss: 0.5766 - val_acc: 0.6829\n",
            "Epoch 311/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5670 - acc: 0.7149 - val_loss: 0.5761 - val_acc: 0.6829\n",
            "Epoch 312/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5665 - acc: 0.7169 - val_loss: 0.5758 - val_acc: 0.6911\n",
            "Epoch 313/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5662 - acc: 0.7169 - val_loss: 0.5752 - val_acc: 0.6829\n",
            "Epoch 314/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5655 - acc: 0.7169 - val_loss: 0.5749 - val_acc: 0.6911\n",
            "Epoch 315/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5651 - acc: 0.7169 - val_loss: 0.5745 - val_acc: 0.6911\n",
            "Epoch 316/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5646 - acc: 0.7149 - val_loss: 0.5741 - val_acc: 0.6911\n",
            "Epoch 317/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5642 - acc: 0.7128 - val_loss: 0.5737 - val_acc: 0.6911\n",
            "Epoch 318/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5636 - acc: 0.7149 - val_loss: 0.5733 - val_acc: 0.6911\n",
            "Epoch 319/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5631 - acc: 0.7169 - val_loss: 0.5730 - val_acc: 0.6992\n",
            "Epoch 320/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5626 - acc: 0.7128 - val_loss: 0.5726 - val_acc: 0.6992\n",
            "Epoch 321/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5622 - acc: 0.7169 - val_loss: 0.5723 - val_acc: 0.7154\n",
            "Epoch 322/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5618 - acc: 0.7128 - val_loss: 0.5720 - val_acc: 0.7154\n",
            "Epoch 323/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5611 - acc: 0.7169 - val_loss: 0.5716 - val_acc: 0.7154\n",
            "Epoch 324/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5608 - acc: 0.7128 - val_loss: 0.5712 - val_acc: 0.7154\n",
            "Epoch 325/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5601 - acc: 0.7149 - val_loss: 0.5707 - val_acc: 0.7154\n",
            "Epoch 326/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.5598 - acc: 0.7149 - val_loss: 0.5702 - val_acc: 0.7154\n",
            "Epoch 327/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5595 - acc: 0.7108 - val_loss: 0.5698 - val_acc: 0.7154\n",
            "Epoch 328/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5590 - acc: 0.7128 - val_loss: 0.5693 - val_acc: 0.7154\n",
            "Epoch 329/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5583 - acc: 0.7149 - val_loss: 0.5688 - val_acc: 0.7154\n",
            "Epoch 330/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5577 - acc: 0.7149 - val_loss: 0.5684 - val_acc: 0.7154\n",
            "Epoch 331/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5571 - acc: 0.7169 - val_loss: 0.5681 - val_acc: 0.7154\n",
            "Epoch 332/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5567 - acc: 0.7128 - val_loss: 0.5678 - val_acc: 0.7154\n",
            "Epoch 333/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5563 - acc: 0.7149 - val_loss: 0.5674 - val_acc: 0.7154\n",
            "Epoch 334/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5559 - acc: 0.7149 - val_loss: 0.5671 - val_acc: 0.7236\n",
            "Epoch 335/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5552 - acc: 0.7108 - val_loss: 0.5665 - val_acc: 0.7154\n",
            "Epoch 336/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5548 - acc: 0.7169 - val_loss: 0.5661 - val_acc: 0.7154\n",
            "Epoch 337/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5543 - acc: 0.7169 - val_loss: 0.5659 - val_acc: 0.7236\n",
            "Epoch 338/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5538 - acc: 0.7108 - val_loss: 0.5654 - val_acc: 0.7154\n",
            "Epoch 339/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5535 - acc: 0.7149 - val_loss: 0.5651 - val_acc: 0.7236\n",
            "Epoch 340/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5530 - acc: 0.7149 - val_loss: 0.5648 - val_acc: 0.7317\n",
            "Epoch 341/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5526 - acc: 0.7149 - val_loss: 0.5644 - val_acc: 0.7317\n",
            "Epoch 342/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5518 - acc: 0.7128 - val_loss: 0.5640 - val_acc: 0.7236\n",
            "Epoch 343/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5515 - acc: 0.7108 - val_loss: 0.5635 - val_acc: 0.7236\n",
            "Epoch 344/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5510 - acc: 0.7108 - val_loss: 0.5632 - val_acc: 0.7236\n",
            "Epoch 345/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5504 - acc: 0.7169 - val_loss: 0.5629 - val_acc: 0.7236\n",
            "Epoch 346/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5498 - acc: 0.7108 - val_loss: 0.5623 - val_acc: 0.7154\n",
            "Epoch 347/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5495 - acc: 0.7128 - val_loss: 0.5620 - val_acc: 0.7154\n",
            "Epoch 348/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5490 - acc: 0.7108 - val_loss: 0.5616 - val_acc: 0.7154\n",
            "Epoch 349/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5485 - acc: 0.7128 - val_loss: 0.5611 - val_acc: 0.7154\n",
            "Epoch 350/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5480 - acc: 0.7169 - val_loss: 0.5608 - val_acc: 0.7154\n",
            "Epoch 351/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5476 - acc: 0.7128 - val_loss: 0.5604 - val_acc: 0.7154\n",
            "Epoch 352/1000\n",
            "491/491 [==============================] - 0s 51us/step - loss: 0.5470 - acc: 0.7088 - val_loss: 0.5600 - val_acc: 0.7154\n",
            "Epoch 353/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5465 - acc: 0.7169 - val_loss: 0.5596 - val_acc: 0.7154\n",
            "Epoch 354/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5460 - acc: 0.7169 - val_loss: 0.5592 - val_acc: 0.7154\n",
            "Epoch 355/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5458 - acc: 0.7149 - val_loss: 0.5589 - val_acc: 0.7154\n",
            "Epoch 356/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5451 - acc: 0.7128 - val_loss: 0.5586 - val_acc: 0.7154\n",
            "Epoch 357/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5447 - acc: 0.7149 - val_loss: 0.5584 - val_acc: 0.7236\n",
            "Epoch 358/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5443 - acc: 0.7169 - val_loss: 0.5579 - val_acc: 0.7154\n",
            "Epoch 359/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5438 - acc: 0.7149 - val_loss: 0.5574 - val_acc: 0.7154\n",
            "Epoch 360/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5433 - acc: 0.7149 - val_loss: 0.5572 - val_acc: 0.7154\n",
            "Epoch 361/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5428 - acc: 0.7169 - val_loss: 0.5568 - val_acc: 0.7154\n",
            "Epoch 362/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5423 - acc: 0.7128 - val_loss: 0.5562 - val_acc: 0.7154\n",
            "Epoch 363/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5420 - acc: 0.7189 - val_loss: 0.5558 - val_acc: 0.7154\n",
            "Epoch 364/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5414 - acc: 0.7169 - val_loss: 0.5555 - val_acc: 0.7154\n",
            "Epoch 365/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5408 - acc: 0.7128 - val_loss: 0.5552 - val_acc: 0.7154\n",
            "Epoch 366/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5406 - acc: 0.7189 - val_loss: 0.5547 - val_acc: 0.7154\n",
            "Epoch 367/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5406 - acc: 0.7189 - val_loss: 0.5543 - val_acc: 0.7154\n",
            "Epoch 368/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5395 - acc: 0.7210 - val_loss: 0.5539 - val_acc: 0.7154\n",
            "Epoch 369/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5389 - acc: 0.7210 - val_loss: 0.5537 - val_acc: 0.7154\n",
            "Epoch 370/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5385 - acc: 0.7189 - val_loss: 0.5534 - val_acc: 0.7154\n",
            "Epoch 371/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5382 - acc: 0.7210 - val_loss: 0.5532 - val_acc: 0.7154\n",
            "Epoch 372/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5375 - acc: 0.7169 - val_loss: 0.5528 - val_acc: 0.7154\n",
            "Epoch 373/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5374 - acc: 0.7149 - val_loss: 0.5524 - val_acc: 0.7154\n",
            "Epoch 374/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5367 - acc: 0.7169 - val_loss: 0.5520 - val_acc: 0.7154\n",
            "Epoch 375/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5363 - acc: 0.7210 - val_loss: 0.5519 - val_acc: 0.7073\n",
            "Epoch 376/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5359 - acc: 0.7149 - val_loss: 0.5513 - val_acc: 0.7154\n",
            "Epoch 377/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5354 - acc: 0.7169 - val_loss: 0.5511 - val_acc: 0.7073\n",
            "Epoch 378/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5351 - acc: 0.7169 - val_loss: 0.5507 - val_acc: 0.7073\n",
            "Epoch 379/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5344 - acc: 0.7169 - val_loss: 0.5503 - val_acc: 0.7154\n",
            "Epoch 380/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.5341 - acc: 0.7230 - val_loss: 0.5500 - val_acc: 0.7073\n",
            "Epoch 381/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5336 - acc: 0.7210 - val_loss: 0.5497 - val_acc: 0.7073\n",
            "Epoch 382/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5332 - acc: 0.7230 - val_loss: 0.5496 - val_acc: 0.7073\n",
            "Epoch 383/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.5329 - acc: 0.7169 - val_loss: 0.5493 - val_acc: 0.7073\n",
            "Epoch 384/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5322 - acc: 0.7169 - val_loss: 0.5490 - val_acc: 0.7073\n",
            "Epoch 385/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5316 - acc: 0.7169 - val_loss: 0.5484 - val_acc: 0.7073\n",
            "Epoch 386/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5312 - acc: 0.7210 - val_loss: 0.5483 - val_acc: 0.7073\n",
            "Epoch 387/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5309 - acc: 0.7189 - val_loss: 0.5479 - val_acc: 0.7073\n",
            "Epoch 388/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5304 - acc: 0.7210 - val_loss: 0.5477 - val_acc: 0.7154\n",
            "Epoch 389/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5298 - acc: 0.7169 - val_loss: 0.5475 - val_acc: 0.7154\n",
            "Epoch 390/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5296 - acc: 0.7169 - val_loss: 0.5472 - val_acc: 0.7154\n",
            "Epoch 391/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5292 - acc: 0.7189 - val_loss: 0.5466 - val_acc: 0.7154\n",
            "Epoch 392/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5284 - acc: 0.7189 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 393/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5282 - acc: 0.7189 - val_loss: 0.5458 - val_acc: 0.7154\n",
            "Epoch 394/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.5281 - acc: 0.7189 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 395/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5272 - acc: 0.7210 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 396/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5268 - acc: 0.7169 - val_loss: 0.5450 - val_acc: 0.7154\n",
            "Epoch 397/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5262 - acc: 0.7189 - val_loss: 0.5448 - val_acc: 0.7154\n",
            "Epoch 398/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5258 - acc: 0.7189 - val_loss: 0.5445 - val_acc: 0.7154\n",
            "Epoch 399/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5254 - acc: 0.7210 - val_loss: 0.5443 - val_acc: 0.7154\n",
            "Epoch 400/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5251 - acc: 0.7189 - val_loss: 0.5443 - val_acc: 0.7154\n",
            "Epoch 401/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5249 - acc: 0.7230 - val_loss: 0.5445 - val_acc: 0.7236\n",
            "Epoch 402/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5243 - acc: 0.7251 - val_loss: 0.5439 - val_acc: 0.7236\n",
            "Epoch 403/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5236 - acc: 0.7210 - val_loss: 0.5434 - val_acc: 0.7154\n",
            "Epoch 404/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5232 - acc: 0.7230 - val_loss: 0.5429 - val_acc: 0.7154\n",
            "Epoch 405/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.5230 - acc: 0.7230 - val_loss: 0.5424 - val_acc: 0.7154\n",
            "Epoch 406/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5225 - acc: 0.7210 - val_loss: 0.5420 - val_acc: 0.7154\n",
            "Epoch 407/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5221 - acc: 0.7230 - val_loss: 0.5419 - val_acc: 0.7154\n",
            "Epoch 408/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5220 - acc: 0.7251 - val_loss: 0.5416 - val_acc: 0.7154\n",
            "Epoch 409/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5211 - acc: 0.7230 - val_loss: 0.5413 - val_acc: 0.7154\n",
            "Epoch 410/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5207 - acc: 0.7251 - val_loss: 0.5412 - val_acc: 0.7154\n",
            "Epoch 411/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5201 - acc: 0.7271 - val_loss: 0.5410 - val_acc: 0.7154\n",
            "Epoch 412/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5197 - acc: 0.7271 - val_loss: 0.5407 - val_acc: 0.7154\n",
            "Epoch 413/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5195 - acc: 0.7291 - val_loss: 0.5402 - val_acc: 0.7154\n",
            "Epoch 414/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5190 - acc: 0.7291 - val_loss: 0.5401 - val_acc: 0.7154\n",
            "Epoch 415/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5186 - acc: 0.7271 - val_loss: 0.5400 - val_acc: 0.7154\n",
            "Epoch 416/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5183 - acc: 0.7271 - val_loss: 0.5396 - val_acc: 0.7154\n",
            "Epoch 417/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5181 - acc: 0.7312 - val_loss: 0.5389 - val_acc: 0.7154\n",
            "Epoch 418/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5173 - acc: 0.7291 - val_loss: 0.5388 - val_acc: 0.7154\n",
            "Epoch 419/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5172 - acc: 0.7332 - val_loss: 0.5385 - val_acc: 0.7154\n",
            "Epoch 420/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5165 - acc: 0.7271 - val_loss: 0.5384 - val_acc: 0.7154\n",
            "Epoch 421/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5160 - acc: 0.7312 - val_loss: 0.5383 - val_acc: 0.7154\n",
            "Epoch 422/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5157 - acc: 0.7291 - val_loss: 0.5380 - val_acc: 0.7154\n",
            "Epoch 423/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5162 - acc: 0.7312 - val_loss: 0.5377 - val_acc: 0.7154\n",
            "Epoch 424/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5148 - acc: 0.7291 - val_loss: 0.5374 - val_acc: 0.7154\n",
            "Epoch 425/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5143 - acc: 0.7352 - val_loss: 0.5372 - val_acc: 0.7154\n",
            "Epoch 426/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5138 - acc: 0.7373 - val_loss: 0.5368 - val_acc: 0.7154\n",
            "Epoch 427/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5141 - acc: 0.7312 - val_loss: 0.5366 - val_acc: 0.7154\n",
            "Epoch 428/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5137 - acc: 0.7332 - val_loss: 0.5363 - val_acc: 0.7154\n",
            "Epoch 429/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5129 - acc: 0.7332 - val_loss: 0.5361 - val_acc: 0.7154\n",
            "Epoch 430/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5124 - acc: 0.7413 - val_loss: 0.5359 - val_acc: 0.7154\n",
            "Epoch 431/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5121 - acc: 0.7332 - val_loss: 0.5359 - val_acc: 0.7236\n",
            "Epoch 432/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5115 - acc: 0.7413 - val_loss: 0.5354 - val_acc: 0.7154\n",
            "Epoch 433/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5111 - acc: 0.7352 - val_loss: 0.5351 - val_acc: 0.7154\n",
            "Epoch 434/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5107 - acc: 0.7352 - val_loss: 0.5349 - val_acc: 0.7154\n",
            "Epoch 435/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5103 - acc: 0.7413 - val_loss: 0.5345 - val_acc: 0.7236\n",
            "Epoch 436/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5099 - acc: 0.7373 - val_loss: 0.5343 - val_acc: 0.7154\n",
            "Epoch 437/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5096 - acc: 0.7352 - val_loss: 0.5343 - val_acc: 0.7236\n",
            "Epoch 438/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5093 - acc: 0.7373 - val_loss: 0.5342 - val_acc: 0.7317\n",
            "Epoch 439/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5091 - acc: 0.7373 - val_loss: 0.5341 - val_acc: 0.7317\n",
            "Epoch 440/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5087 - acc: 0.7434 - val_loss: 0.5338 - val_acc: 0.7317\n",
            "Epoch 441/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5082 - acc: 0.7413 - val_loss: 0.5335 - val_acc: 0.7317\n",
            "Epoch 442/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5079 - acc: 0.7434 - val_loss: 0.5332 - val_acc: 0.7317\n",
            "Epoch 443/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5075 - acc: 0.7454 - val_loss: 0.5329 - val_acc: 0.7236\n",
            "Epoch 444/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5069 - acc: 0.7393 - val_loss: 0.5327 - val_acc: 0.7236\n",
            "Epoch 445/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5066 - acc: 0.7393 - val_loss: 0.5326 - val_acc: 0.7317\n",
            "Epoch 446/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5063 - acc: 0.7413 - val_loss: 0.5325 - val_acc: 0.7236\n",
            "Epoch 447/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5057 - acc: 0.7475 - val_loss: 0.5319 - val_acc: 0.7236\n",
            "Epoch 448/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5054 - acc: 0.7413 - val_loss: 0.5315 - val_acc: 0.7236\n",
            "Epoch 449/1000\n",
            "491/491 [==============================] - 0s 51us/step - loss: 0.5051 - acc: 0.7413 - val_loss: 0.5315 - val_acc: 0.7236\n",
            "Epoch 450/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5046 - acc: 0.7454 - val_loss: 0.5311 - val_acc: 0.7236\n",
            "Epoch 451/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5047 - acc: 0.7413 - val_loss: 0.5309 - val_acc: 0.7236\n",
            "Epoch 452/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5038 - acc: 0.7413 - val_loss: 0.5306 - val_acc: 0.7236\n",
            "Epoch 453/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5040 - acc: 0.7393 - val_loss: 0.5306 - val_acc: 0.7236\n",
            "Epoch 454/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5033 - acc: 0.7454 - val_loss: 0.5303 - val_acc: 0.7236\n",
            "Epoch 455/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5029 - acc: 0.7373 - val_loss: 0.5307 - val_acc: 0.7236\n",
            "Epoch 456/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5026 - acc: 0.7454 - val_loss: 0.5304 - val_acc: 0.7236\n",
            "Epoch 457/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5022 - acc: 0.7434 - val_loss: 0.5302 - val_acc: 0.7236\n",
            "Epoch 458/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5017 - acc: 0.7475 - val_loss: 0.5299 - val_acc: 0.7236\n",
            "Epoch 459/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5017 - acc: 0.7475 - val_loss: 0.5295 - val_acc: 0.7154\n",
            "Epoch 460/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5011 - acc: 0.7454 - val_loss: 0.5293 - val_acc: 0.7154\n",
            "Epoch 461/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5007 - acc: 0.7454 - val_loss: 0.5291 - val_acc: 0.7236\n",
            "Epoch 462/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5009 - acc: 0.7454 - val_loss: 0.5291 - val_acc: 0.7236\n",
            "Epoch 463/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5003 - acc: 0.7373 - val_loss: 0.5292 - val_acc: 0.7236\n",
            "Epoch 464/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.4999 - acc: 0.7454 - val_loss: 0.5285 - val_acc: 0.7154\n",
            "Epoch 465/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.4994 - acc: 0.7454 - val_loss: 0.5284 - val_acc: 0.7154\n",
            "Epoch 466/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4991 - acc: 0.7434 - val_loss: 0.5281 - val_acc: 0.7154\n",
            "Epoch 467/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.4989 - acc: 0.7454 - val_loss: 0.5282 - val_acc: 0.7236\n",
            "Epoch 468/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4983 - acc: 0.7475 - val_loss: 0.5283 - val_acc: 0.7236\n",
            "Epoch 469/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4983 - acc: 0.7475 - val_loss: 0.5277 - val_acc: 0.7154\n",
            "Epoch 470/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4977 - acc: 0.7434 - val_loss: 0.5276 - val_acc: 0.7154\n",
            "Epoch 471/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4981 - acc: 0.7413 - val_loss: 0.5276 - val_acc: 0.7154\n",
            "Epoch 472/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4971 - acc: 0.7454 - val_loss: 0.5276 - val_acc: 0.7236\n",
            "Epoch 473/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4967 - acc: 0.7475 - val_loss: 0.5272 - val_acc: 0.7154\n",
            "Epoch 474/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4963 - acc: 0.7454 - val_loss: 0.5269 - val_acc: 0.7154\n",
            "Epoch 475/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4963 - acc: 0.7434 - val_loss: 0.5266 - val_acc: 0.7236\n",
            "Epoch 476/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4959 - acc: 0.7475 - val_loss: 0.5265 - val_acc: 0.7236\n",
            "Epoch 477/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4952 - acc: 0.7454 - val_loss: 0.5264 - val_acc: 0.7073\n",
            "Epoch 478/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4953 - acc: 0.7454 - val_loss: 0.5263 - val_acc: 0.7073\n",
            "Epoch 479/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4948 - acc: 0.7434 - val_loss: 0.5259 - val_acc: 0.7236\n",
            "Epoch 480/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4945 - acc: 0.7454 - val_loss: 0.5260 - val_acc: 0.7236\n",
            "Epoch 481/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4942 - acc: 0.7434 - val_loss: 0.5258 - val_acc: 0.7154\n",
            "Epoch 482/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4936 - acc: 0.7413 - val_loss: 0.5257 - val_acc: 0.7236\n",
            "Epoch 483/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4934 - acc: 0.7413 - val_loss: 0.5256 - val_acc: 0.7236\n",
            "Epoch 484/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4931 - acc: 0.7454 - val_loss: 0.5253 - val_acc: 0.7236\n",
            "Epoch 485/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.4932 - acc: 0.7413 - val_loss: 0.5254 - val_acc: 0.7236\n",
            "Epoch 486/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4925 - acc: 0.7413 - val_loss: 0.5253 - val_acc: 0.7236\n",
            "Epoch 487/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4921 - acc: 0.7413 - val_loss: 0.5251 - val_acc: 0.7317\n",
            "Epoch 488/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4920 - acc: 0.7413 - val_loss: 0.5247 - val_acc: 0.7236\n",
            "Epoch 489/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.4919 - acc: 0.7413 - val_loss: 0.5247 - val_acc: 0.7236\n",
            "Epoch 490/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4914 - acc: 0.7434 - val_loss: 0.5245 - val_acc: 0.7236\n",
            "Epoch 491/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4914 - acc: 0.7454 - val_loss: 0.5245 - val_acc: 0.7398\n",
            "Epoch 492/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4908 - acc: 0.7434 - val_loss: 0.5241 - val_acc: 0.7236\n",
            "Epoch 493/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4908 - acc: 0.7413 - val_loss: 0.5241 - val_acc: 0.7236\n",
            "Epoch 494/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4903 - acc: 0.7413 - val_loss: 0.5238 - val_acc: 0.7236\n",
            "Epoch 495/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4899 - acc: 0.7413 - val_loss: 0.5237 - val_acc: 0.7236\n",
            "Epoch 496/1000\n",
            "491/491 [==============================] - 0s 52us/step - loss: 0.4896 - acc: 0.7413 - val_loss: 0.5237 - val_acc: 0.7236\n",
            "Epoch 497/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.4892 - acc: 0.7515 - val_loss: 0.5234 - val_acc: 0.7236\n",
            "Epoch 498/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4898 - acc: 0.7413 - val_loss: 0.5232 - val_acc: 0.7236\n",
            "Epoch 499/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4890 - acc: 0.7475 - val_loss: 0.5234 - val_acc: 0.7317\n",
            "Epoch 500/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4884 - acc: 0.7454 - val_loss: 0.5233 - val_acc: 0.7317\n",
            "Epoch 501/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4885 - acc: 0.7454 - val_loss: 0.5230 - val_acc: 0.7236\n",
            "Epoch 502/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4879 - acc: 0.7454 - val_loss: 0.5229 - val_acc: 0.7317\n",
            "Epoch 503/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.4878 - acc: 0.7454 - val_loss: 0.5228 - val_acc: 0.7317\n",
            "Epoch 504/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4873 - acc: 0.7454 - val_loss: 0.5227 - val_acc: 0.7317\n",
            "Epoch 505/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4875 - acc: 0.7454 - val_loss: 0.5225 - val_acc: 0.7317\n",
            "Epoch 506/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4870 - acc: 0.7454 - val_loss: 0.5225 - val_acc: 0.7317\n",
            "Epoch 507/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4870 - acc: 0.7434 - val_loss: 0.5224 - val_acc: 0.7317\n",
            "Epoch 508/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4864 - acc: 0.7475 - val_loss: 0.5222 - val_acc: 0.7317\n",
            "Epoch 509/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4858 - acc: 0.7454 - val_loss: 0.5222 - val_acc: 0.7317\n",
            "Epoch 510/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4855 - acc: 0.7495 - val_loss: 0.5219 - val_acc: 0.7317\n",
            "Epoch 511/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4859 - acc: 0.7413 - val_loss: 0.5218 - val_acc: 0.7317\n",
            "Epoch 512/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.4861 - acc: 0.7495 - val_loss: 0.5215 - val_acc: 0.7236\n",
            "Epoch 513/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4847 - acc: 0.7434 - val_loss: 0.5215 - val_acc: 0.7236\n",
            "Epoch 514/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4848 - acc: 0.7495 - val_loss: 0.5214 - val_acc: 0.7236\n",
            "Epoch 515/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4845 - acc: 0.7495 - val_loss: 0.5218 - val_acc: 0.7317\n",
            "Epoch 516/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.4845 - acc: 0.7515 - val_loss: 0.5212 - val_acc: 0.7236\n",
            "Epoch 517/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4837 - acc: 0.7454 - val_loss: 0.5213 - val_acc: 0.7317\n",
            "Epoch 518/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4836 - acc: 0.7434 - val_loss: 0.5212 - val_acc: 0.7317\n",
            "Epoch 519/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4831 - acc: 0.7434 - val_loss: 0.5214 - val_acc: 0.7480\n",
            "Epoch 520/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4830 - acc: 0.7515 - val_loss: 0.5210 - val_acc: 0.7317\n",
            "Epoch 521/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4828 - acc: 0.7495 - val_loss: 0.5210 - val_acc: 0.7398\n",
            "Epoch 522/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4828 - acc: 0.7495 - val_loss: 0.5208 - val_acc: 0.7398\n",
            "Epoch 523/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4829 - acc: 0.7495 - val_loss: 0.5206 - val_acc: 0.7317\n",
            "Epoch 524/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4820 - acc: 0.7475 - val_loss: 0.5205 - val_acc: 0.7236\n",
            "Epoch 525/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4822 - acc: 0.7495 - val_loss: 0.5204 - val_acc: 0.7236\n",
            "Epoch 526/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4818 - acc: 0.7475 - val_loss: 0.5204 - val_acc: 0.7398\n",
            "Epoch 527/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4816 - acc: 0.7475 - val_loss: 0.5204 - val_acc: 0.7398\n",
            "Epoch 528/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4809 - acc: 0.7536 - val_loss: 0.5203 - val_acc: 0.7398\n",
            "Epoch 529/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4810 - acc: 0.7536 - val_loss: 0.5205 - val_acc: 0.7480\n",
            "Epoch 530/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4809 - acc: 0.7556 - val_loss: 0.5200 - val_acc: 0.7398\n",
            "Epoch 531/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.4805 - acc: 0.7495 - val_loss: 0.5200 - val_acc: 0.7398\n",
            "Epoch 532/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4802 - acc: 0.7515 - val_loss: 0.5199 - val_acc: 0.7398\n",
            "Epoch 533/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4800 - acc: 0.7515 - val_loss: 0.5201 - val_acc: 0.7480\n",
            "Epoch 534/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4800 - acc: 0.7536 - val_loss: 0.5199 - val_acc: 0.7398\n",
            "Epoch 535/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4801 - acc: 0.7515 - val_loss: 0.5199 - val_acc: 0.7480\n",
            "Epoch 536/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4791 - acc: 0.7536 - val_loss: 0.5199 - val_acc: 0.7480\n",
            "Epoch 537/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4794 - acc: 0.7556 - val_loss: 0.5197 - val_acc: 0.7398\n",
            "Epoch 538/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4788 - acc: 0.7536 - val_loss: 0.5199 - val_acc: 0.7480\n",
            "Epoch 539/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4787 - acc: 0.7515 - val_loss: 0.5194 - val_acc: 0.7398\n",
            "Epoch 540/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.4786 - acc: 0.7536 - val_loss: 0.5194 - val_acc: 0.7398\n",
            "Epoch 541/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4786 - acc: 0.7576 - val_loss: 0.5193 - val_acc: 0.7398\n",
            "Epoch 542/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4784 - acc: 0.7576 - val_loss: 0.5192 - val_acc: 0.7317\n",
            "Epoch 543/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4779 - acc: 0.7556 - val_loss: 0.5192 - val_acc: 0.7398\n",
            "Epoch 544/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4781 - acc: 0.7556 - val_loss: 0.5191 - val_acc: 0.7398\n",
            "Epoch 545/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.4778 - acc: 0.7597 - val_loss: 0.5191 - val_acc: 0.7398\n",
            "Epoch 546/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4773 - acc: 0.7597 - val_loss: 0.5190 - val_acc: 0.7398\n",
            "Epoch 547/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4769 - acc: 0.7556 - val_loss: 0.5190 - val_acc: 0.7398\n",
            "Epoch 548/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4767 - acc: 0.7576 - val_loss: 0.5189 - val_acc: 0.7398\n",
            "Epoch 549/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4768 - acc: 0.7597 - val_loss: 0.5189 - val_acc: 0.7398\n",
            "Epoch 550/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4766 - acc: 0.7576 - val_loss: 0.5189 - val_acc: 0.7480\n",
            "Epoch 551/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4766 - acc: 0.7597 - val_loss: 0.5190 - val_acc: 0.7480\n",
            "Epoch 552/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4764 - acc: 0.7556 - val_loss: 0.5186 - val_acc: 0.7398\n",
            "Epoch 553/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4763 - acc: 0.7576 - val_loss: 0.5186 - val_acc: 0.7398\n",
            "Epoch 554/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4753 - acc: 0.7597 - val_loss: 0.5186 - val_acc: 0.7398\n",
            "Epoch 555/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4752 - acc: 0.7597 - val_loss: 0.5187 - val_acc: 0.7480\n",
            "Epoch 556/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4758 - acc: 0.7576 - val_loss: 0.5194 - val_acc: 0.7561\n",
            "Epoch 557/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4752 - acc: 0.7617 - val_loss: 0.5186 - val_acc: 0.7480\n",
            "Epoch 558/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4749 - acc: 0.7617 - val_loss: 0.5186 - val_acc: 0.7480\n",
            "Epoch 559/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4748 - acc: 0.7617 - val_loss: 0.5184 - val_acc: 0.7480\n",
            "Epoch 560/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4746 - acc: 0.7617 - val_loss: 0.5183 - val_acc: 0.7398\n",
            "Epoch 561/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4742 - acc: 0.7658 - val_loss: 0.5183 - val_acc: 0.7480\n",
            "Epoch 562/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4741 - acc: 0.7617 - val_loss: 0.5184 - val_acc: 0.7480\n",
            "Epoch 563/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4741 - acc: 0.7658 - val_loss: 0.5182 - val_acc: 0.7398\n",
            "Epoch 564/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4743 - acc: 0.7637 - val_loss: 0.5181 - val_acc: 0.7398\n",
            "Epoch 565/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4738 - acc: 0.7597 - val_loss: 0.5183 - val_acc: 0.7480\n",
            "Epoch 566/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4737 - acc: 0.7658 - val_loss: 0.5189 - val_acc: 0.7561\n",
            "Epoch 567/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4733 - acc: 0.7637 - val_loss: 0.5185 - val_acc: 0.7480\n",
            "Epoch 568/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4735 - acc: 0.7637 - val_loss: 0.5184 - val_acc: 0.7480\n",
            "Epoch 569/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4730 - acc: 0.7637 - val_loss: 0.5185 - val_acc: 0.7480\n",
            "Epoch 570/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4724 - acc: 0.7678 - val_loss: 0.5180 - val_acc: 0.7480\n",
            "Epoch 571/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4730 - acc: 0.7719 - val_loss: 0.5178 - val_acc: 0.7398\n",
            "Epoch 572/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4722 - acc: 0.7678 - val_loss: 0.5178 - val_acc: 0.7398\n",
            "Epoch 573/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4722 - acc: 0.7637 - val_loss: 0.5182 - val_acc: 0.7480\n",
            "Epoch 574/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4721 - acc: 0.7739 - val_loss: 0.5181 - val_acc: 0.7480\n",
            "Epoch 575/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4721 - acc: 0.7678 - val_loss: 0.5179 - val_acc: 0.7480\n",
            "Epoch 576/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4722 - acc: 0.7678 - val_loss: 0.5177 - val_acc: 0.7398\n",
            "Epoch 577/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4715 - acc: 0.7699 - val_loss: 0.5177 - val_acc: 0.7398\n",
            "Epoch 578/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4716 - acc: 0.7699 - val_loss: 0.5177 - val_acc: 0.7480\n",
            "Epoch 579/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4710 - acc: 0.7637 - val_loss: 0.5177 - val_acc: 0.7480\n",
            "Epoch 580/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4707 - acc: 0.7678 - val_loss: 0.5177 - val_acc: 0.7480\n",
            "Epoch 581/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4709 - acc: 0.7719 - val_loss: 0.5177 - val_acc: 0.7398\n",
            "Epoch 582/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4709 - acc: 0.7719 - val_loss: 0.5176 - val_acc: 0.7398\n",
            "Epoch 583/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4706 - acc: 0.7658 - val_loss: 0.5178 - val_acc: 0.7480\n",
            "Epoch 584/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4704 - acc: 0.7719 - val_loss: 0.5175 - val_acc: 0.7480\n",
            "Epoch 585/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4703 - acc: 0.7637 - val_loss: 0.5175 - val_acc: 0.7480\n",
            "Epoch 586/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4699 - acc: 0.7699 - val_loss: 0.5175 - val_acc: 0.7480\n",
            "Epoch 587/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4701 - acc: 0.7719 - val_loss: 0.5176 - val_acc: 0.7480\n",
            "Epoch 588/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4698 - acc: 0.7699 - val_loss: 0.5176 - val_acc: 0.7480\n",
            "Epoch 589/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4701 - acc: 0.7699 - val_loss: 0.5179 - val_acc: 0.7561\n",
            "Epoch 590/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4696 - acc: 0.7739 - val_loss: 0.5179 - val_acc: 0.7561\n",
            "Epoch 591/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4696 - acc: 0.7739 - val_loss: 0.5179 - val_acc: 0.7561\n",
            "Epoch 592/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4690 - acc: 0.7719 - val_loss: 0.5177 - val_acc: 0.7561\n",
            "Epoch 593/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4698 - acc: 0.7719 - val_loss: 0.5175 - val_acc: 0.7480\n",
            "Epoch 594/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4691 - acc: 0.7719 - val_loss: 0.5174 - val_acc: 0.7480\n",
            "Epoch 595/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4687 - acc: 0.7739 - val_loss: 0.5174 - val_acc: 0.7480\n",
            "Epoch 596/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4683 - acc: 0.7678 - val_loss: 0.5173 - val_acc: 0.7480\n",
            "Epoch 597/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4685 - acc: 0.7658 - val_loss: 0.5176 - val_acc: 0.7561\n",
            "Epoch 598/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4682 - acc: 0.7760 - val_loss: 0.5174 - val_acc: 0.7480\n",
            "Epoch 599/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4694 - acc: 0.7658 - val_loss: 0.5174 - val_acc: 0.7561\n",
            "Epoch 600/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4681 - acc: 0.7719 - val_loss: 0.5173 - val_acc: 0.7480\n",
            "Epoch 601/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4675 - acc: 0.7678 - val_loss: 0.5174 - val_acc: 0.7561\n",
            "Epoch 602/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4679 - acc: 0.7739 - val_loss: 0.5173 - val_acc: 0.7561\n",
            "Epoch 603/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4674 - acc: 0.7780 - val_loss: 0.5173 - val_acc: 0.7480\n",
            "Epoch 604/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4676 - acc: 0.7658 - val_loss: 0.5175 - val_acc: 0.7561\n",
            "Epoch 605/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4676 - acc: 0.7739 - val_loss: 0.5173 - val_acc: 0.7561\n",
            "Epoch 606/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4675 - acc: 0.7699 - val_loss: 0.5172 - val_acc: 0.7480\n",
            "Epoch 607/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4673 - acc: 0.7760 - val_loss: 0.5172 - val_acc: 0.7480\n",
            "Epoch 608/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4670 - acc: 0.7739 - val_loss: 0.5172 - val_acc: 0.7561\n",
            "Epoch 609/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4671 - acc: 0.7739 - val_loss: 0.5174 - val_acc: 0.7480\n",
            "Epoch 610/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4667 - acc: 0.7699 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 611/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4664 - acc: 0.7719 - val_loss: 0.5172 - val_acc: 0.7561\n",
            "Epoch 612/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4665 - acc: 0.7719 - val_loss: 0.5172 - val_acc: 0.7561\n",
            "Epoch 613/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4667 - acc: 0.7739 - val_loss: 0.5171 - val_acc: 0.7561\n",
            "Epoch 614/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4664 - acc: 0.7760 - val_loss: 0.5171 - val_acc: 0.7561\n",
            "Epoch 615/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4658 - acc: 0.7739 - val_loss: 0.5172 - val_acc: 0.7561\n",
            "Epoch 616/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4658 - acc: 0.7760 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 617/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4659 - acc: 0.7739 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 618/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4658 - acc: 0.7800 - val_loss: 0.5171 - val_acc: 0.7561\n",
            "Epoch 619/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.4657 - acc: 0.7760 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 620/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4656 - acc: 0.7760 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 621/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4652 - acc: 0.7719 - val_loss: 0.5172 - val_acc: 0.7561\n",
            "Epoch 622/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4655 - acc: 0.7760 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 623/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4657 - acc: 0.7800 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 624/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4651 - acc: 0.7760 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 625/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4651 - acc: 0.7821 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 626/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4651 - acc: 0.7760 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 627/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4651 - acc: 0.7800 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 628/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 629/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.4645 - acc: 0.7780 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 630/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4648 - acc: 0.7719 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 631/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.4643 - acc: 0.7821 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 632/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4648 - acc: 0.7739 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 633/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4642 - acc: 0.7760 - val_loss: 0.5171 - val_acc: 0.7642\n",
            "Epoch 634/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4641 - acc: 0.7719 - val_loss: 0.5173 - val_acc: 0.7642\n",
            "Epoch 635/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4640 - acc: 0.7780 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 636/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4640 - acc: 0.7780 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 637/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4637 - acc: 0.7780 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 638/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4638 - acc: 0.7800 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 639/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4633 - acc: 0.7760 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 640/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.4639 - acc: 0.7780 - val_loss: 0.5173 - val_acc: 0.7642\n",
            "Epoch 641/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4643 - acc: 0.7800 - val_loss: 0.5172 - val_acc: 0.7642\n",
            "Epoch 642/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4636 - acc: 0.7699 - val_loss: 0.5174 - val_acc: 0.7642\n",
            "Epoch 643/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.4631 - acc: 0.7739 - val_loss: 0.5177 - val_acc: 0.7561\n",
            "Epoch 644/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4638 - acc: 0.7739 - val_loss: 0.5173 - val_acc: 0.7642\n",
            "Epoch 645/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4630 - acc: 0.7800 - val_loss: 0.5173 - val_acc: 0.7642\n",
            "Epoch 646/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4634 - acc: 0.7780 - val_loss: 0.5175 - val_acc: 0.7561\n",
            "Epoch 647/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4627 - acc: 0.7739 - val_loss: 0.5173 - val_acc: 0.7642\n",
            "Epoch 648/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4625 - acc: 0.7780 - val_loss: 0.5173 - val_acc: 0.7642\n",
            "Epoch 649/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4630 - acc: 0.7739 - val_loss: 0.5173 - val_acc: 0.7642\n",
            "Epoch 650/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4630 - acc: 0.7739 - val_loss: 0.5174 - val_acc: 0.7642\n",
            "Epoch 651/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4625 - acc: 0.7760 - val_loss: 0.5174 - val_acc: 0.7642\n",
            "Epoch 652/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4622 - acc: 0.7739 - val_loss: 0.5174 - val_acc: 0.7642\n",
            "Epoch 653/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4626 - acc: 0.7821 - val_loss: 0.5174 - val_acc: 0.7642\n",
            "Epoch 654/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4630 - acc: 0.7800 - val_loss: 0.5174 - val_acc: 0.7642\n",
            "Epoch 655/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.4623 - acc: 0.7800 - val_loss: 0.5175 - val_acc: 0.7642\n",
            "Epoch 656/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4626 - acc: 0.7780 - val_loss: 0.5175 - val_acc: 0.7642\n",
            "Epoch 657/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4615 - acc: 0.7841 - val_loss: 0.5175 - val_acc: 0.7724\n",
            "Epoch 658/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4621 - acc: 0.7760 - val_loss: 0.5174 - val_acc: 0.7642\n",
            "Epoch 659/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4617 - acc: 0.7841 - val_loss: 0.5175 - val_acc: 0.7642\n",
            "Epoch 660/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4619 - acc: 0.7800 - val_loss: 0.5175 - val_acc: 0.7642\n",
            "Epoch 661/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4619 - acc: 0.7800 - val_loss: 0.5175 - val_acc: 0.7642\n",
            "Epoch 662/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4618 - acc: 0.7780 - val_loss: 0.5176 - val_acc: 0.7642\n",
            "Epoch 663/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4612 - acc: 0.7800 - val_loss: 0.5175 - val_acc: 0.7642\n",
            "Epoch 664/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4618 - acc: 0.7800 - val_loss: 0.5176 - val_acc: 0.7642\n",
            "Epoch 665/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4609 - acc: 0.7800 - val_loss: 0.5175 - val_acc: 0.7642\n",
            "Epoch 666/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4608 - acc: 0.7821 - val_loss: 0.5176 - val_acc: 0.7642\n",
            "Epoch 667/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4608 - acc: 0.7800 - val_loss: 0.5177 - val_acc: 0.7642\n",
            "Epoch 668/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4613 - acc: 0.7841 - val_loss: 0.5176 - val_acc: 0.7642\n",
            "Epoch 669/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4612 - acc: 0.7821 - val_loss: 0.5178 - val_acc: 0.7561\n",
            "Epoch 670/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4606 - acc: 0.7821 - val_loss: 0.5176 - val_acc: 0.7642\n",
            "Epoch 671/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4609 - acc: 0.7821 - val_loss: 0.5177 - val_acc: 0.7642\n",
            "Epoch 672/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4607 - acc: 0.7821 - val_loss: 0.5177 - val_acc: 0.7642\n",
            "Epoch 673/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4604 - acc: 0.7841 - val_loss: 0.5177 - val_acc: 0.7642\n",
            "Epoch 674/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4607 - acc: 0.7780 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 675/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4608 - acc: 0.7821 - val_loss: 0.5180 - val_acc: 0.7642\n",
            "Epoch 676/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4611 - acc: 0.7800 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 677/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4609 - acc: 0.7800 - val_loss: 0.5179 - val_acc: 0.7642\n",
            "Epoch 678/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4603 - acc: 0.7739 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 679/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4599 - acc: 0.7800 - val_loss: 0.5180 - val_acc: 0.7642\n",
            "Epoch 680/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4609 - acc: 0.7800 - val_loss: 0.5182 - val_acc: 0.7724\n",
            "Epoch 681/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4602 - acc: 0.7841 - val_loss: 0.5179 - val_acc: 0.7642\n",
            "Epoch 682/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4600 - acc: 0.7882 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 683/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4605 - acc: 0.7821 - val_loss: 0.5177 - val_acc: 0.7642\n",
            "Epoch 684/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4598 - acc: 0.7821 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 685/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4603 - acc: 0.7821 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 686/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4594 - acc: 0.7821 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 687/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4606 - acc: 0.7821 - val_loss: 0.5177 - val_acc: 0.7642\n",
            "Epoch 688/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4592 - acc: 0.7841 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 689/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4592 - acc: 0.7780 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 690/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4607 - acc: 0.7841 - val_loss: 0.5181 - val_acc: 0.7642\n",
            "Epoch 691/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4594 - acc: 0.7760 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 692/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4593 - acc: 0.7841 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 693/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4594 - acc: 0.7800 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 694/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4590 - acc: 0.7882 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 695/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4593 - acc: 0.7821 - val_loss: 0.5178 - val_acc: 0.7642\n",
            "Epoch 696/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4592 - acc: 0.7800 - val_loss: 0.5179 - val_acc: 0.7642\n",
            "Epoch 697/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4593 - acc: 0.7882 - val_loss: 0.5179 - val_acc: 0.7642\n",
            "Epoch 698/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4586 - acc: 0.7780 - val_loss: 0.5180 - val_acc: 0.7642\n",
            "Epoch 699/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4586 - acc: 0.7780 - val_loss: 0.5179 - val_acc: 0.7642\n",
            "Epoch 700/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4588 - acc: 0.7780 - val_loss: 0.5179 - val_acc: 0.7642\n",
            "Epoch 701/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4600 - acc: 0.7800 - val_loss: 0.5184 - val_acc: 0.7724\n",
            "Epoch 702/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4588 - acc: 0.7760 - val_loss: 0.5179 - val_acc: 0.7642\n",
            "Epoch 703/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4586 - acc: 0.7800 - val_loss: 0.5180 - val_acc: 0.7642\n",
            "Epoch 704/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4581 - acc: 0.7841 - val_loss: 0.5179 - val_acc: 0.7561\n",
            "Epoch 705/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4581 - acc: 0.7800 - val_loss: 0.5179 - val_acc: 0.7642\n",
            "Epoch 706/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4579 - acc: 0.7800 - val_loss: 0.5180 - val_acc: 0.7561\n",
            "Epoch 707/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4588 - acc: 0.7821 - val_loss: 0.5180 - val_acc: 0.7642\n",
            "Epoch 708/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4581 - acc: 0.7800 - val_loss: 0.5181 - val_acc: 0.7642\n",
            "Epoch 709/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4582 - acc: 0.7841 - val_loss: 0.5181 - val_acc: 0.7642\n",
            "Epoch 710/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4578 - acc: 0.7821 - val_loss: 0.5181 - val_acc: 0.7642\n",
            "Epoch 711/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4577 - acc: 0.7821 - val_loss: 0.5181 - val_acc: 0.7642\n",
            "Epoch 712/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4582 - acc: 0.7800 - val_loss: 0.5181 - val_acc: 0.7642\n",
            "Epoch 713/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4577 - acc: 0.7800 - val_loss: 0.5181 - val_acc: 0.7642\n",
            "Epoch 714/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4578 - acc: 0.7780 - val_loss: 0.5182 - val_acc: 0.7642\n",
            "Epoch 715/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4574 - acc: 0.7882 - val_loss: 0.5182 - val_acc: 0.7642\n",
            "Epoch 716/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4578 - acc: 0.7780 - val_loss: 0.5182 - val_acc: 0.7561\n",
            "Epoch 717/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4579 - acc: 0.7841 - val_loss: 0.5182 - val_acc: 0.7642\n",
            "Epoch 718/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4583 - acc: 0.7780 - val_loss: 0.5183 - val_acc: 0.7642\n",
            "Epoch 719/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4573 - acc: 0.7760 - val_loss: 0.5183 - val_acc: 0.7561\n",
            "Epoch 720/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4580 - acc: 0.7780 - val_loss: 0.5186 - val_acc: 0.7724\n",
            "Epoch 721/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4577 - acc: 0.7841 - val_loss: 0.5183 - val_acc: 0.7642\n",
            "Epoch 722/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4572 - acc: 0.7780 - val_loss: 0.5184 - val_acc: 0.7642\n",
            "Epoch 723/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4574 - acc: 0.7862 - val_loss: 0.5185 - val_acc: 0.7642\n",
            "Epoch 724/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4572 - acc: 0.7800 - val_loss: 0.5183 - val_acc: 0.7642\n",
            "Epoch 725/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4575 - acc: 0.7780 - val_loss: 0.5184 - val_acc: 0.7642\n",
            "Epoch 726/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4571 - acc: 0.7760 - val_loss: 0.5184 - val_acc: 0.7642\n",
            "Epoch 727/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4570 - acc: 0.7821 - val_loss: 0.5186 - val_acc: 0.7724\n",
            "Epoch 728/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4574 - acc: 0.7882 - val_loss: 0.5184 - val_acc: 0.7642\n",
            "Epoch 729/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4567 - acc: 0.7821 - val_loss: 0.5183 - val_acc: 0.7561\n",
            "Epoch 730/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4569 - acc: 0.7800 - val_loss: 0.5183 - val_acc: 0.7642\n",
            "Epoch 731/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4573 - acc: 0.7821 - val_loss: 0.5184 - val_acc: 0.7642\n",
            "Epoch 732/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4571 - acc: 0.7821 - val_loss: 0.5184 - val_acc: 0.7642\n",
            "Epoch 733/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4568 - acc: 0.7841 - val_loss: 0.5184 - val_acc: 0.7642\n",
            "Epoch 734/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4563 - acc: 0.7780 - val_loss: 0.5184 - val_acc: 0.7642\n",
            "Epoch 735/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4563 - acc: 0.7841 - val_loss: 0.5185 - val_acc: 0.7642\n",
            "Epoch 736/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4572 - acc: 0.7862 - val_loss: 0.5185 - val_acc: 0.7561\n",
            "Epoch 737/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4563 - acc: 0.7800 - val_loss: 0.5185 - val_acc: 0.7642\n",
            "Epoch 738/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4570 - acc: 0.7800 - val_loss: 0.5185 - val_acc: 0.7642\n",
            "Epoch 739/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4566 - acc: 0.7800 - val_loss: 0.5186 - val_acc: 0.7642\n",
            "Epoch 740/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4563 - acc: 0.7841 - val_loss: 0.5186 - val_acc: 0.7642\n",
            "Epoch 741/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4564 - acc: 0.7862 - val_loss: 0.5186 - val_acc: 0.7642\n",
            "Epoch 742/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4565 - acc: 0.7780 - val_loss: 0.5185 - val_acc: 0.7642\n",
            "Epoch 743/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4569 - acc: 0.7780 - val_loss: 0.5185 - val_acc: 0.7642\n",
            "Epoch 744/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4562 - acc: 0.7760 - val_loss: 0.5186 - val_acc: 0.7724\n",
            "Epoch 745/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4560 - acc: 0.7862 - val_loss: 0.5185 - val_acc: 0.7642\n",
            "Epoch 746/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4561 - acc: 0.7821 - val_loss: 0.5186 - val_acc: 0.7642\n",
            "Epoch 747/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4562 - acc: 0.7800 - val_loss: 0.5186 - val_acc: 0.7561\n",
            "Epoch 748/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4569 - acc: 0.7862 - val_loss: 0.5186 - val_acc: 0.7561\n",
            "Epoch 749/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4557 - acc: 0.7821 - val_loss: 0.5187 - val_acc: 0.7642\n",
            "Epoch 750/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4560 - acc: 0.7800 - val_loss: 0.5186 - val_acc: 0.7561\n",
            "Epoch 751/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4559 - acc: 0.7800 - val_loss: 0.5187 - val_acc: 0.7642\n",
            "Epoch 752/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4560 - acc: 0.7841 - val_loss: 0.5189 - val_acc: 0.7642\n",
            "Epoch 753/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4560 - acc: 0.7760 - val_loss: 0.5192 - val_acc: 0.7642\n",
            "Epoch 754/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4565 - acc: 0.7841 - val_loss: 0.5200 - val_acc: 0.7480\n",
            "Epoch 755/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4559 - acc: 0.7780 - val_loss: 0.5191 - val_acc: 0.7642\n",
            "Epoch 756/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4554 - acc: 0.7800 - val_loss: 0.5190 - val_acc: 0.7642\n",
            "Epoch 757/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4565 - acc: 0.7780 - val_loss: 0.5190 - val_acc: 0.7642\n",
            "Epoch 758/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4559 - acc: 0.7760 - val_loss: 0.5188 - val_acc: 0.7642\n",
            "Epoch 759/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4556 - acc: 0.7841 - val_loss: 0.5188 - val_acc: 0.7642\n",
            "Epoch 760/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4557 - acc: 0.7780 - val_loss: 0.5189 - val_acc: 0.7724\n",
            "Epoch 761/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4557 - acc: 0.7760 - val_loss: 0.5188 - val_acc: 0.7642\n",
            "Epoch 762/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4551 - acc: 0.7800 - val_loss: 0.5188 - val_acc: 0.7642\n",
            "Epoch 763/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4551 - acc: 0.7760 - val_loss: 0.5188 - val_acc: 0.7642\n",
            "Epoch 764/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4556 - acc: 0.7780 - val_loss: 0.5189 - val_acc: 0.7561\n",
            "Epoch 765/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4552 - acc: 0.7800 - val_loss: 0.5189 - val_acc: 0.7561\n",
            "Epoch 766/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4554 - acc: 0.7800 - val_loss: 0.5189 - val_acc: 0.7642\n",
            "Epoch 767/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4556 - acc: 0.7821 - val_loss: 0.5189 - val_acc: 0.7642\n",
            "Epoch 768/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4554 - acc: 0.7841 - val_loss: 0.5189 - val_acc: 0.7642\n",
            "Epoch 769/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4552 - acc: 0.7800 - val_loss: 0.5189 - val_acc: 0.7561\n",
            "Epoch 770/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4559 - acc: 0.7821 - val_loss: 0.5193 - val_acc: 0.7724\n",
            "Epoch 771/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4556 - acc: 0.7780 - val_loss: 0.5189 - val_acc: 0.7642\n",
            "Epoch 772/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4549 - acc: 0.7821 - val_loss: 0.5190 - val_acc: 0.7642\n",
            "Epoch 773/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4550 - acc: 0.7862 - val_loss: 0.5192 - val_acc: 0.7724\n",
            "Epoch 774/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4551 - acc: 0.7800 - val_loss: 0.5191 - val_acc: 0.7724\n",
            "Epoch 775/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4547 - acc: 0.7800 - val_loss: 0.5192 - val_acc: 0.7642\n",
            "Epoch 776/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4548 - acc: 0.7821 - val_loss: 0.5192 - val_acc: 0.7642\n",
            "Epoch 777/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4549 - acc: 0.7760 - val_loss: 0.5194 - val_acc: 0.7642\n",
            "Epoch 778/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4556 - acc: 0.7821 - val_loss: 0.5191 - val_acc: 0.7642\n",
            "Epoch 779/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4548 - acc: 0.7800 - val_loss: 0.5192 - val_acc: 0.7724\n",
            "Epoch 780/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4556 - acc: 0.7841 - val_loss: 0.5192 - val_acc: 0.7642\n",
            "Epoch 781/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4546 - acc: 0.7780 - val_loss: 0.5193 - val_acc: 0.7724\n",
            "Epoch 782/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4545 - acc: 0.7800 - val_loss: 0.5192 - val_acc: 0.7642\n",
            "Epoch 783/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4552 - acc: 0.7780 - val_loss: 0.5192 - val_acc: 0.7642\n",
            "Epoch 784/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4544 - acc: 0.7800 - val_loss: 0.5193 - val_acc: 0.7642\n",
            "Epoch 785/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4551 - acc: 0.7821 - val_loss: 0.5193 - val_acc: 0.7724\n",
            "Epoch 786/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4544 - acc: 0.7821 - val_loss: 0.5197 - val_acc: 0.7561\n",
            "Epoch 787/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4546 - acc: 0.7800 - val_loss: 0.5194 - val_acc: 0.7642\n",
            "Epoch 788/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4541 - acc: 0.7800 - val_loss: 0.5197 - val_acc: 0.7561\n",
            "Epoch 789/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4544 - acc: 0.7821 - val_loss: 0.5199 - val_acc: 0.7561\n",
            "Epoch 790/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4552 - acc: 0.7800 - val_loss: 0.5200 - val_acc: 0.7561\n",
            "Epoch 791/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4547 - acc: 0.7841 - val_loss: 0.5201 - val_acc: 0.7561\n",
            "Epoch 792/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4549 - acc: 0.7800 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 793/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4539 - acc: 0.7821 - val_loss: 0.5194 - val_acc: 0.7642\n",
            "Epoch 794/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4546 - acc: 0.7780 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 795/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4543 - acc: 0.7841 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 796/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4536 - acc: 0.7821 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 797/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4538 - acc: 0.7821 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 798/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4547 - acc: 0.7841 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 799/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4548 - acc: 0.7821 - val_loss: 0.5194 - val_acc: 0.7724\n",
            "Epoch 800/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4541 - acc: 0.7780 - val_loss: 0.5194 - val_acc: 0.7642\n",
            "Epoch 801/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4544 - acc: 0.7882 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 802/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4537 - acc: 0.7841 - val_loss: 0.5195 - val_acc: 0.7724\n",
            "Epoch 803/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4537 - acc: 0.7800 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 804/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4536 - acc: 0.7800 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 805/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4534 - acc: 0.7821 - val_loss: 0.5194 - val_acc: 0.7724\n",
            "Epoch 806/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.4534 - acc: 0.7821 - val_loss: 0.5194 - val_acc: 0.7642\n",
            "Epoch 807/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4536 - acc: 0.7800 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 808/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4540 - acc: 0.7800 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 809/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4536 - acc: 0.7780 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 810/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4532 - acc: 0.7800 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 811/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4532 - acc: 0.7821 - val_loss: 0.5197 - val_acc: 0.7642\n",
            "Epoch 812/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4532 - acc: 0.7821 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 813/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4547 - acc: 0.7760 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 814/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4540 - acc: 0.7800 - val_loss: 0.5196 - val_acc: 0.7724\n",
            "Epoch 815/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4534 - acc: 0.7800 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 816/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4536 - acc: 0.7821 - val_loss: 0.5195 - val_acc: 0.7642\n",
            "Epoch 817/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4528 - acc: 0.7821 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 818/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4546 - acc: 0.7821 - val_loss: 0.5201 - val_acc: 0.7561\n",
            "Epoch 819/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4542 - acc: 0.7821 - val_loss: 0.5197 - val_acc: 0.7642\n",
            "Epoch 820/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4530 - acc: 0.7841 - val_loss: 0.5197 - val_acc: 0.7642\n",
            "Epoch 821/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4535 - acc: 0.7841 - val_loss: 0.5196 - val_acc: 0.7642\n",
            "Epoch 822/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4541 - acc: 0.7780 - val_loss: 0.5197 - val_acc: 0.7642\n",
            "Epoch 823/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4528 - acc: 0.7821 - val_loss: 0.5197 - val_acc: 0.7642\n",
            "Epoch 824/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4533 - acc: 0.7780 - val_loss: 0.5197 - val_acc: 0.7724\n",
            "Epoch 825/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4531 - acc: 0.7760 - val_loss: 0.5197 - val_acc: 0.7642\n",
            "Epoch 826/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4534 - acc: 0.7780 - val_loss: 0.5200 - val_acc: 0.7561\n",
            "Epoch 827/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.4532 - acc: 0.7800 - val_loss: 0.5200 - val_acc: 0.7642\n",
            "Epoch 828/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4534 - acc: 0.7821 - val_loss: 0.5197 - val_acc: 0.7724\n",
            "Epoch 829/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4525 - acc: 0.7821 - val_loss: 0.5197 - val_acc: 0.7642\n",
            "Epoch 830/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4526 - acc: 0.7800 - val_loss: 0.5197 - val_acc: 0.7642\n",
            "Epoch 831/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4537 - acc: 0.7780 - val_loss: 0.5204 - val_acc: 0.7724\n",
            "Epoch 832/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4537 - acc: 0.7841 - val_loss: 0.5199 - val_acc: 0.7642\n",
            "Epoch 833/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4525 - acc: 0.7800 - val_loss: 0.5199 - val_acc: 0.7724\n",
            "Epoch 834/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4529 - acc: 0.7800 - val_loss: 0.5201 - val_acc: 0.7561\n",
            "Epoch 835/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4537 - acc: 0.7800 - val_loss: 0.5198 - val_acc: 0.7642\n",
            "Epoch 836/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4529 - acc: 0.7780 - val_loss: 0.5198 - val_acc: 0.7642\n",
            "Epoch 837/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4526 - acc: 0.7841 - val_loss: 0.5200 - val_acc: 0.7642\n",
            "Epoch 838/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4530 - acc: 0.7821 - val_loss: 0.5199 - val_acc: 0.7724\n",
            "Epoch 839/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4528 - acc: 0.7760 - val_loss: 0.5198 - val_acc: 0.7642\n",
            "Epoch 840/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4527 - acc: 0.7800 - val_loss: 0.5199 - val_acc: 0.7642\n",
            "Epoch 841/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4524 - acc: 0.7800 - val_loss: 0.5201 - val_acc: 0.7561\n",
            "Epoch 842/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4529 - acc: 0.7800 - val_loss: 0.5201 - val_acc: 0.7561\n",
            "Epoch 843/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4526 - acc: 0.7841 - val_loss: 0.5202 - val_acc: 0.7561\n",
            "Epoch 844/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4533 - acc: 0.7821 - val_loss: 0.5199 - val_acc: 0.7642\n",
            "Epoch 845/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4521 - acc: 0.7800 - val_loss: 0.5205 - val_acc: 0.7561\n",
            "Epoch 846/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4531 - acc: 0.7841 - val_loss: 0.5200 - val_acc: 0.7561\n",
            "Epoch 847/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4518 - acc: 0.7841 - val_loss: 0.5200 - val_acc: 0.7724\n",
            "Epoch 848/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4521 - acc: 0.7821 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 849/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4525 - acc: 0.7862 - val_loss: 0.5201 - val_acc: 0.7724\n",
            "Epoch 850/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4525 - acc: 0.7821 - val_loss: 0.5201 - val_acc: 0.7724\n",
            "Epoch 851/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4522 - acc: 0.7800 - val_loss: 0.5201 - val_acc: 0.7724\n",
            "Epoch 852/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4530 - acc: 0.7760 - val_loss: 0.5200 - val_acc: 0.7724\n",
            "Epoch 853/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4520 - acc: 0.7800 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 854/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4521 - acc: 0.7841 - val_loss: 0.5202 - val_acc: 0.7642\n",
            "Epoch 855/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4533 - acc: 0.7780 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 856/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4518 - acc: 0.7821 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 857/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4516 - acc: 0.7800 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 858/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4520 - acc: 0.7780 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 859/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4528 - acc: 0.7821 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 860/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4526 - acc: 0.7800 - val_loss: 0.5204 - val_acc: 0.7561\n",
            "Epoch 861/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4521 - acc: 0.7862 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 862/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4519 - acc: 0.7821 - val_loss: 0.5205 - val_acc: 0.7561\n",
            "Epoch 863/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4521 - acc: 0.7821 - val_loss: 0.5200 - val_acc: 0.7642\n",
            "Epoch 864/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4522 - acc: 0.7821 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 865/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4521 - acc: 0.7800 - val_loss: 0.5200 - val_acc: 0.7642\n",
            "Epoch 866/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4516 - acc: 0.7841 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 867/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.4525 - acc: 0.7821 - val_loss: 0.5200 - val_acc: 0.7642\n",
            "Epoch 868/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4517 - acc: 0.7862 - val_loss: 0.5200 - val_acc: 0.7724\n",
            "Epoch 869/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4520 - acc: 0.7821 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 870/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4516 - acc: 0.7862 - val_loss: 0.5200 - val_acc: 0.7642\n",
            "Epoch 871/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4524 - acc: 0.7882 - val_loss: 0.5210 - val_acc: 0.7642\n",
            "Epoch 872/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4516 - acc: 0.7841 - val_loss: 0.5202 - val_acc: 0.7642\n",
            "Epoch 873/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4516 - acc: 0.7841 - val_loss: 0.5204 - val_acc: 0.7561\n",
            "Epoch 874/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4521 - acc: 0.7841 - val_loss: 0.5202 - val_acc: 0.7561\n",
            "Epoch 875/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4510 - acc: 0.7841 - val_loss: 0.5205 - val_acc: 0.7561\n",
            "Epoch 876/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4513 - acc: 0.7841 - val_loss: 0.5202 - val_acc: 0.7724\n",
            "Epoch 877/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4518 - acc: 0.7841 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 878/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4523 - acc: 0.7841 - val_loss: 0.5203 - val_acc: 0.7561\n",
            "Epoch 879/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4519 - acc: 0.7780 - val_loss: 0.5204 - val_acc: 0.7561\n",
            "Epoch 880/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4514 - acc: 0.7821 - val_loss: 0.5207 - val_acc: 0.7561\n",
            "Epoch 881/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4514 - acc: 0.7862 - val_loss: 0.5204 - val_acc: 0.7561\n",
            "Epoch 882/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4519 - acc: 0.7841 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 883/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4518 - acc: 0.7862 - val_loss: 0.5202 - val_acc: 0.7642\n",
            "Epoch 884/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4513 - acc: 0.7821 - val_loss: 0.5204 - val_acc: 0.7561\n",
            "Epoch 885/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4516 - acc: 0.7841 - val_loss: 0.5201 - val_acc: 0.7642\n",
            "Epoch 886/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4512 - acc: 0.7821 - val_loss: 0.5201 - val_acc: 0.7724\n",
            "Epoch 887/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4510 - acc: 0.7882 - val_loss: 0.5202 - val_acc: 0.7724\n",
            "Epoch 888/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4507 - acc: 0.7800 - val_loss: 0.5207 - val_acc: 0.7561\n",
            "Epoch 889/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4507 - acc: 0.7841 - val_loss: 0.5202 - val_acc: 0.7642\n",
            "Epoch 890/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4519 - acc: 0.7841 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 891/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4508 - acc: 0.7800 - val_loss: 0.5203 - val_acc: 0.7561\n",
            "Epoch 892/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4507 - acc: 0.7821 - val_loss: 0.5203 - val_acc: 0.7561\n",
            "Epoch 893/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4518 - acc: 0.7821 - val_loss: 0.5202 - val_acc: 0.7642\n",
            "Epoch 894/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4507 - acc: 0.7862 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 895/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4506 - acc: 0.7862 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 896/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4508 - acc: 0.7862 - val_loss: 0.5203 - val_acc: 0.7724\n",
            "Epoch 897/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4508 - acc: 0.7862 - val_loss: 0.5205 - val_acc: 0.7561\n",
            "Epoch 898/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4505 - acc: 0.7841 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 899/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4507 - acc: 0.7862 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 900/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4506 - acc: 0.7841 - val_loss: 0.5205 - val_acc: 0.7561\n",
            "Epoch 901/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4505 - acc: 0.7821 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 902/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4510 - acc: 0.7780 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 903/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4508 - acc: 0.7821 - val_loss: 0.5205 - val_acc: 0.7561\n",
            "Epoch 904/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4513 - acc: 0.7882 - val_loss: 0.5203 - val_acc: 0.7642\n",
            "Epoch 905/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4510 - acc: 0.7821 - val_loss: 0.5205 - val_acc: 0.7561\n",
            "Epoch 906/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.4517 - acc: 0.7821 - val_loss: 0.5213 - val_acc: 0.7642\n",
            "Epoch 907/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4505 - acc: 0.7882 - val_loss: 0.5211 - val_acc: 0.7642\n",
            "Epoch 908/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4506 - acc: 0.7862 - val_loss: 0.5211 - val_acc: 0.7642\n",
            "Epoch 909/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4513 - acc: 0.7841 - val_loss: 0.5210 - val_acc: 0.7642\n",
            "Epoch 910/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.4508 - acc: 0.7862 - val_loss: 0.5211 - val_acc: 0.7642\n",
            "Epoch 911/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4503 - acc: 0.7841 - val_loss: 0.5204 - val_acc: 0.7724\n",
            "Epoch 912/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4515 - acc: 0.7841 - val_loss: 0.5205 - val_acc: 0.7642\n",
            "Epoch 913/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4505 - acc: 0.7862 - val_loss: 0.5204 - val_acc: 0.7642\n",
            "Epoch 914/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4500 - acc: 0.7821 - val_loss: 0.5205 - val_acc: 0.7642\n",
            "Epoch 915/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4503 - acc: 0.7841 - val_loss: 0.5207 - val_acc: 0.7561\n",
            "Epoch 916/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4503 - acc: 0.7862 - val_loss: 0.5207 - val_acc: 0.7561\n",
            "Epoch 917/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4507 - acc: 0.7841 - val_loss: 0.5209 - val_acc: 0.7561\n",
            "Epoch 918/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4508 - acc: 0.7821 - val_loss: 0.5207 - val_acc: 0.7561\n",
            "Epoch 919/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4502 - acc: 0.7821 - val_loss: 0.5207 - val_acc: 0.7561\n",
            "Epoch 920/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4498 - acc: 0.7800 - val_loss: 0.5206 - val_acc: 0.7724\n",
            "Epoch 921/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4500 - acc: 0.7862 - val_loss: 0.5208 - val_acc: 0.7561\n",
            "Epoch 922/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4504 - acc: 0.7862 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 923/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4501 - acc: 0.7862 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 924/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4501 - acc: 0.7821 - val_loss: 0.5212 - val_acc: 0.7642\n",
            "Epoch 925/1000\n",
            "491/491 [==============================] - 0s 50us/step - loss: 0.4498 - acc: 0.7841 - val_loss: 0.5206 - val_acc: 0.7642\n",
            "Epoch 926/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4506 - acc: 0.7862 - val_loss: 0.5207 - val_acc: 0.7642\n",
            "Epoch 927/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4496 - acc: 0.7821 - val_loss: 0.5207 - val_acc: 0.7642\n",
            "Epoch 928/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4497 - acc: 0.7821 - val_loss: 0.5207 - val_acc: 0.7561\n",
            "Epoch 929/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4494 - acc: 0.7862 - val_loss: 0.5206 - val_acc: 0.7724\n",
            "Epoch 930/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4496 - acc: 0.7862 - val_loss: 0.5207 - val_acc: 0.7561\n",
            "Epoch 931/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4495 - acc: 0.7882 - val_loss: 0.5207 - val_acc: 0.7642\n",
            "Epoch 932/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4509 - acc: 0.7862 - val_loss: 0.5207 - val_acc: 0.7724\n",
            "Epoch 933/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4506 - acc: 0.7862 - val_loss: 0.5209 - val_acc: 0.7561\n",
            "Epoch 934/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4496 - acc: 0.7841 - val_loss: 0.5207 - val_acc: 0.7724\n",
            "Epoch 935/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4497 - acc: 0.7841 - val_loss: 0.5207 - val_acc: 0.7724\n",
            "Epoch 936/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4499 - acc: 0.7862 - val_loss: 0.5209 - val_acc: 0.7561\n",
            "Epoch 937/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4493 - acc: 0.7841 - val_loss: 0.5208 - val_acc: 0.7561\n",
            "Epoch 938/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4494 - acc: 0.7882 - val_loss: 0.5209 - val_acc: 0.7561\n",
            "Epoch 939/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4494 - acc: 0.7862 - val_loss: 0.5208 - val_acc: 0.7561\n",
            "Epoch 940/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4493 - acc: 0.7862 - val_loss: 0.5209 - val_acc: 0.7561\n",
            "Epoch 941/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4494 - acc: 0.7862 - val_loss: 0.5209 - val_acc: 0.7642\n",
            "Epoch 942/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4497 - acc: 0.7882 - val_loss: 0.5211 - val_acc: 0.7561\n",
            "Epoch 943/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4500 - acc: 0.7841 - val_loss: 0.5210 - val_acc: 0.7561\n",
            "Epoch 944/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4489 - acc: 0.7821 - val_loss: 0.5209 - val_acc: 0.7561\n",
            "Epoch 945/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4496 - acc: 0.7862 - val_loss: 0.5209 - val_acc: 0.7724\n",
            "Epoch 946/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4503 - acc: 0.7882 - val_loss: 0.5209 - val_acc: 0.7642\n",
            "Epoch 947/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4502 - acc: 0.7800 - val_loss: 0.5210 - val_acc: 0.7642\n",
            "Epoch 948/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4495 - acc: 0.7841 - val_loss: 0.5210 - val_acc: 0.7642\n",
            "Epoch 949/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4497 - acc: 0.7821 - val_loss: 0.5209 - val_acc: 0.7724\n",
            "Epoch 950/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4491 - acc: 0.7882 - val_loss: 0.5210 - val_acc: 0.7642\n",
            "Epoch 951/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4495 - acc: 0.7923 - val_loss: 0.5210 - val_acc: 0.7561\n",
            "Epoch 952/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4495 - acc: 0.7862 - val_loss: 0.5213 - val_acc: 0.7642\n",
            "Epoch 953/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4491 - acc: 0.7780 - val_loss: 0.5211 - val_acc: 0.7561\n",
            "Epoch 954/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4501 - acc: 0.7882 - val_loss: 0.5210 - val_acc: 0.7642\n",
            "Epoch 955/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4493 - acc: 0.7841 - val_loss: 0.5211 - val_acc: 0.7561\n",
            "Epoch 956/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4489 - acc: 0.7882 - val_loss: 0.5211 - val_acc: 0.7561\n",
            "Epoch 957/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4495 - acc: 0.7841 - val_loss: 0.5210 - val_acc: 0.7724\n",
            "Epoch 958/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4493 - acc: 0.7882 - val_loss: 0.5213 - val_acc: 0.7642\n",
            "Epoch 959/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4496 - acc: 0.7841 - val_loss: 0.5210 - val_acc: 0.7642\n",
            "Epoch 960/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4495 - acc: 0.7841 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 961/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4499 - acc: 0.7841 - val_loss: 0.5214 - val_acc: 0.7561\n",
            "Epoch 962/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4505 - acc: 0.7841 - val_loss: 0.5213 - val_acc: 0.7642\n",
            "Epoch 963/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4491 - acc: 0.7841 - val_loss: 0.5211 - val_acc: 0.7642\n",
            "Epoch 964/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4495 - acc: 0.7841 - val_loss: 0.5211 - val_acc: 0.7642\n",
            "Epoch 965/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4492 - acc: 0.7882 - val_loss: 0.5218 - val_acc: 0.7642\n",
            "Epoch 966/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4489 - acc: 0.7862 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 967/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4484 - acc: 0.7841 - val_loss: 0.5211 - val_acc: 0.7561\n",
            "Epoch 968/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4492 - acc: 0.7841 - val_loss: 0.5212 - val_acc: 0.7561\n",
            "Epoch 969/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4490 - acc: 0.7841 - val_loss: 0.5213 - val_acc: 0.7561\n",
            "Epoch 970/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4491 - acc: 0.7862 - val_loss: 0.5213 - val_acc: 0.7642\n",
            "Epoch 971/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4483 - acc: 0.7902 - val_loss: 0.5213 - val_acc: 0.7561\n",
            "Epoch 972/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4497 - acc: 0.7882 - val_loss: 0.5213 - val_acc: 0.7561\n",
            "Epoch 973/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4494 - acc: 0.7841 - val_loss: 0.5212 - val_acc: 0.7561\n",
            "Epoch 974/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4488 - acc: 0.7882 - val_loss: 0.5214 - val_acc: 0.7561\n",
            "Epoch 975/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4489 - acc: 0.7923 - val_loss: 0.5214 - val_acc: 0.7561\n",
            "Epoch 976/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4485 - acc: 0.7841 - val_loss: 0.5214 - val_acc: 0.7561\n",
            "Epoch 977/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4490 - acc: 0.7882 - val_loss: 0.5219 - val_acc: 0.7642\n",
            "Epoch 978/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4486 - acc: 0.7882 - val_loss: 0.5216 - val_acc: 0.7561\n",
            "Epoch 979/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4486 - acc: 0.7862 - val_loss: 0.5219 - val_acc: 0.7642\n",
            "Epoch 980/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4495 - acc: 0.7841 - val_loss: 0.5216 - val_acc: 0.7642\n",
            "Epoch 981/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4486 - acc: 0.7862 - val_loss: 0.5215 - val_acc: 0.7642\n",
            "Epoch 982/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4485 - acc: 0.7841 - val_loss: 0.5213 - val_acc: 0.7724\n",
            "Epoch 983/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4482 - acc: 0.7902 - val_loss: 0.5214 - val_acc: 0.7561\n",
            "Epoch 984/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4484 - acc: 0.7841 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 985/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4488 - acc: 0.7882 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 986/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4482 - acc: 0.7862 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 987/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4482 - acc: 0.7862 - val_loss: 0.5214 - val_acc: 0.7561\n",
            "Epoch 988/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4483 - acc: 0.7882 - val_loss: 0.5215 - val_acc: 0.7642\n",
            "Epoch 989/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4481 - acc: 0.7841 - val_loss: 0.5216 - val_acc: 0.7642\n",
            "Epoch 990/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4486 - acc: 0.7862 - val_loss: 0.5215 - val_acc: 0.7561\n",
            "Epoch 991/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4483 - acc: 0.7862 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 992/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4484 - acc: 0.7841 - val_loss: 0.5215 - val_acc: 0.7642\n",
            "Epoch 993/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4486 - acc: 0.7841 - val_loss: 0.5215 - val_acc: 0.7642\n",
            "Epoch 994/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4484 - acc: 0.7862 - val_loss: 0.5214 - val_acc: 0.7642\n",
            "Epoch 995/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4487 - acc: 0.7800 - val_loss: 0.5214 - val_acc: 0.7724\n",
            "Epoch 996/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4486 - acc: 0.7882 - val_loss: 0.5215 - val_acc: 0.7642\n",
            "Epoch 997/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4484 - acc: 0.7882 - val_loss: 0.5215 - val_acc: 0.7642\n",
            "Epoch 998/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4489 - acc: 0.7882 - val_loss: 0.5221 - val_acc: 0.7642\n",
            "Epoch 999/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4484 - acc: 0.7841 - val_loss: 0.5215 - val_acc: 0.7480\n",
            "Epoch 1000/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4478 - acc: 0.7902 - val_loss: 0.5215 - val_acc: 0.7642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96z9lSgUL5Yt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "61f1a1d0-f278-43d6-ba44-e897d63f0a63"
      },
      "source": [
        "#visualize the training loss and the validation loss to see if the model is overfitting\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAE0CAYAAAC8ZD1pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTwghhM2R5CeEEMLmSPITQghhc/4fN7S+Dz2jKuAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Ldcj4bMsSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "217c7340-7c3c-42d8-f41e-c3474c3b390b"
      },
      "source": [
        "#visualize the training accuracy and the validation accuracy to see if the model is overfitting\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcgAAAE0CAYAAACo8aOIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXB8YgnAM0xd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a604c25c-b6bf-4fa1-8b26-0213a9c6a6f1"
      },
      "source": [
        "#Make a prediction & print the actual values\n",
        "prediction = model.predict(X_test)\n",
        "prediction  = [1 if y>=0.5 else 0 for y in prediction] #Threshold\n",
        "print(prediction)\n",
        "print(y_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1]\n",
            "[0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n",
            " 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyvhdN4JM35X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "de2b72a0-0887-4716-d7a4-140ab21bc533"
      },
      "source": [
        "#Evaluate the model on the training data set\n",
        "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
        "pred = model.predict(X_train)\n",
        "pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n",
        "print(classification_report(y_train ,pred ))\n",
        "print('Confusion Matrix: \\n',confusion_matrix(y_train,pred))\n",
        "print()\n",
        "print('Accuracy: ', accuracy_score(y_train,pred))\n",
        "print()\n",
        "\n",
        "#Print the predictions\n",
        "#print('Predicted value: ',model.predict(X_train))\n",
        "\n",
        "#Print Actual Label\n",
        "#print('Actual value: ',y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.88      0.84       398\n",
            "         1.0       0.74      0.60      0.66       216\n",
            "\n",
            "    accuracy                           0.79       614\n",
            "   macro avg       0.77      0.74      0.75       614\n",
            "weighted avg       0.78      0.79      0.78       614\n",
            "\n",
            "Confusion Matrix: \n",
            " [[352  46]\n",
            " [ 86 130]]\n",
            "\n",
            "Accuracy:  0.7850162866449512\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsbDyT9pM8ux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "e66f89d2-02a6-4a38-edfa-8d2253f54d73"
      },
      "source": [
        "#Evaluate the model on the test data set\n",
        "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
        "pred = model.predict(X_test)\n",
        "pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n",
        "print(classification_report(y_test ,pred ))\n",
        "print('Confusion Matrix: \\n',confusion_matrix(y_test,pred))\n",
        "print()\n",
        "print('Accuracy: ', accuracy_score(y_test,pred))\n",
        "print()\n",
        "\n",
        "#Print the predictions\n",
        "#print('Predicted value: ',model.predict(X_test))\n",
        "\n",
        "#Print Actual Label\n",
        "#print('Actual value: ',y_test)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.85      0.84       102\n",
            "         1.0       0.70      0.67      0.69        52\n",
            "\n",
            "    accuracy                           0.79       154\n",
            "   macro avg       0.77      0.76      0.77       154\n",
            "weighted avg       0.79      0.79      0.79       154\n",
            "\n",
            "Confusion Matrix: \n",
            " [[87 15]\n",
            " [17 35]]\n",
            "\n",
            "Accuracy:  0.7922077922077922\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zEgik4-NBRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0bbf458-3e9a-49b2-8511-9e5609dfb891"
      },
      "source": [
        "#Evaluate the test data set\n",
        "\n",
        "#The reason why we have the index 1 after the model.evaluate function is because\n",
        "#the function returns the loss as the first element and the accuracy as the \n",
        "#second element. To only output the accuracy, simply access the second element \n",
        "#(which is indexed by 1, since the first element starts its indexing from 0).\n",
        "model.evaluate(X_test, y_test)[1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "154/154 [==============================] - 0s 36us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7922077937559648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wun0g5tFNKfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}